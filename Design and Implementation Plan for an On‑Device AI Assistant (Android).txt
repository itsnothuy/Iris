Design and Implementation Plan for an On‑Device AI Assistant (Android)
1. Architecture & Module Design
We will structure the app into clear layers, minimizing JNI complexity by grouping related native calls and using high-level abstractions in Kotlin/Java where possible:
* LLM Inference Engine (Core): A native module wrapping llama.cpp for running the main large language model (LLM). Exposes simple JNI methods to load a model, prompt it, and generate tokens (with streaming support). This layer also handles KV-cache management and supports plug-in backends (CPU, OpenCL, Vulkan) based on device. It will include a lightweight scheduler to manage multiple model instances (e.g. main LLM vs. embedding model vs. guard model) without conflicting threads.
* Hardware Abstraction Layer (Acceleration): A small Kotlin singleton or native helper that detects device chipset and selects the optimal backend. At app start, it checks the SoC/GPU type (e.g. via Build.SOC_MANUFACTURER or vendor strings) and configures llama.cpp accordingly (OpenCL on Adreno, Vulkan on Mali, or CPU as fallback). This module also wraps any Hexagon/QNN offload - e.g. loading ONNX Runtime with QNN EP or a TFLite delegate - for smaller models. The goal is to keep hardware-specific logic in one place for easy maintenance.
* Retrieval & Knowledge Base (RAG layer): Manages on-device storage of embeddings and documents. Provides interfaces to index new data (e.g. chunk text and generate embeddings via a small model) and query it (find relevant docs given a query embedding). Implementation will use SQLite with the sqlite-vec extension for simplicity and portability[1][2]. Alternatively, a pluggable interface can allow using ObjectBox's Vector DB if needed for larger indexes or HNSW-based speed. This layer runs mostly in Kotlin (using standard SQLite or ObjectBox APIs) and calls into JNI only to compute embeddings via the small model.
* Multimodal I/O Layer: Handles audio and image processing. This includes an ASR module (using whisper.cpp or Faster-Whisper via CTranslate2) exposed through JNI for offline speech-to-text, and a TTS module (could use Android TTS for now, since it's on-device). For vision, it interfaces with MediaPipe (via the MediaPipe Tasks API or an on-device ML model) to perform image analysis (object detection, OCR, etc.), then formats those results as text for the LLM. If llama.cpp's libmtmd (multimodal library) is stable, we can integrate it to allow the LLM to directly consume visual embeddings[3] (requiring loading an image projector GGUF model alongside the LLM). This layer will ensure each modality (mic, camera) is permission-gated and processed on-device.
* Tool Use & Agent Layer: Implements a local "function calling" mechanism. We define a set of tool schemas (e.g. create_calendar_event(title, datetime), send_sms(number, message), open_app(name), set_alarm(time), run_tasker(taskName)) and provide the LLM with a JSON-based or function signature format in prompts. When the LLM outputs a function call (e.g. a JSON with { "tool": "send_sms", "args": { ... }}), the agent layer parses it and maps it to Android actions. Mapping is done via Intents or direct API calls: e.g. send_sms uses SmsManager or Intent.ACTION_SENDTO, create_calendar_event uses Intent.ACTION_INSERT (Calendar provider), etc. This layer will run on the UI thread or a work thread in Kotlin, but always with a confirmation UI. Before executing a tool, it will display a review dialog ("Allow assistant to send this SMS?") for safety. After confirmation, it executes the intent and returns the result (or success/failure) back to the LLM if needed. It will also register App Shortcuts for frequently used actions - e.g. if the user often asks "remind me to...", we add a dynamic shortcut for the "Add Reminder" action, making it accessible outside the app.
* Safety & Guardrail Layer: A lightweight content moderation module. Before each LLM invocation, it uses a small classifier model (e.g. Meta's Llama Prompt Guard 2 86M[4] or even the 22M variant for speed) to scan user prompts for disallowed content or jailbreak attempts. Similarly, after generation, it can scan the LLM's output for policy violations (using either the same model or a different one tuned for harmful content). This layer will enforce a local policy: e.g. refusing requests about self-harm, and stripping or warning about toxic outputs. It exposes a simple API to the UI layer - e.g. Safety.check(prompt) returning categories or a risk score - so the UI can decide to block, warn, or proceed. All scans are on-device for privacy.
* UI/UX Layer: The Android UI (Activities/Fragments/Views) which ties everything together. Key components include: Model Manager UI (to download/manage GGUF model files, possibly using Play Asset Delivery for large files), Chat interface (with microphone button for voice, attach button for images, etc.), Settings (for toggling safety modes, selecting model, enabling indexing of certain data sources), and the Permission & Privacy UI (explaining what data is kept local, offering "Reset memory" or "Wipe embeddings" actions). The UI communicates with the core via a combination of direct JNI calls (for synchronous tasks like generating one token or running a classifier) and background WorkManager tasks (for longer indexing or model downloads). We will use a foreground service when running long LLM sessions with the screen off (to comply with Android background execution limits).
JNI Complexity Minimization: We reduce JNI overhead by batching calls and using background threads in C++ for heavy loops. For example, generating tokens can run entirely in C++ and invoke a Kotlin callback only for finalized output or intermediate text every N tokens, rather than calling JNI for each token. We'll also leverage memory mapping where possible: e.g. let the llama.cpp C++ code directly write to a shared ByteBuffer or use DirectBuffer for embeddings, to avoid large byte array copies across the JNI boundary.
2. Backends & Hardware Integration (Snapdragon vs. Exynos)
To maximize performance on different chips, we will compile and use multiple backends:
* llama.cpp with OpenCL (Snapdragon/Adreno): We will build llama.cpp enabling the OpenCL backend (i.e. CMake with -DGGML_OPENCL=ON). Qualcomm's contributions have optimized this path for Adreno GPUs[5][6]. We'll use Adreno-specific kernels (ensuring GGML_OPENCL_USE_ADRENO_KERNELS=ON at compile time) and prefer Q4_0 quantization for models on Adreno, since the OpenCL kernels are highly optimized for 4-bit (pure) quantization[7]. Compilation: Because Android NDK doesn't include OpenCL by default, we'll bundle the Khronos OpenCL ICD. Concretely, we fetch OpenCL headers and build the ICD loader for arm64-v8a, then include the resulting libOpenCL.so in our APK and have llama.cpp load it[8][9]. Our Gradle build will package this .so so that on devices with Adreno GPUs, the app can use the included ICD to access the GPU driver. At runtime, our hardware layer will detect Qualcomm (e.g. via Build.HARDWARE containing "qcom" or using Adreno (TM) in android.opengl.GLES20.glGetString(GL10.GL_RENDERER) for a clue) and select the OpenCL backend. We can also query llama.cpp's device list (llama_backend_devices() if available) to ensure an OpenCL device is found. On a Snapdragon 8 Gen1+ device, we expect substantial speedups: e.g. community benchmarks indicate ~5-10 tok/s on 7B models with 4-bit on Adreno 730, vs ~2-3 tok/s on CPU[10][11]. This means real-time or near real-time responses for moderate-length answers on flagship phones.
* llama.cpp with Vulkan (Exynos/Mali or AMD): For Mali GPUs (Exynos devices) and Google Tensor (Pixel devices), we will compile with -DGGML_VULKAN=ON. Vulkan is the fallback GPU path for non-Adreno. It is less specialized per vendor, but still offers GPU acceleration. On Pixel 6/7 (Mali G78) and Pixel 8 (Immortalis or GPU from Samsung/Google), Vulkan should work, though current reports show Vulkan performance may be inconsistent - in some cases slower than CPU for smaller GPUs or poor drivers[12]. We will use Vulkan backend opportunistically: our hardware detection will prefer Vulkan on known capable devices (e.g. newer Mali G710+ or Xclipse GPUs). For older or midrange Mali (which often have driver issues), we allow the user to switch off GPU use if it's slower. The app can run a short benchmark on first run (e.g. generate 16 tokens with Vulkan vs CPU) to automatically decide best default. The Vulkan backend supports a range of quantizations (including Q4_K and others), but not all are equally optimized; we might use Q4_K_M as a default quant for non-Adreno if supported, since it offers a quality boost for minimal speed loss on CPU[13]. On GPU, we'll need to verify if Q4_K_M runs - if not, fall back to a simpler quant like Q4_0 or Q5_0 for GPU. We expect Vulkan to give decent speed-up on high-end Mali (perhaps 2-3× faster than CPU). For example, if a 7B model runs ~2 tok/s on a Exynos 2200 CPU, the Mali GPU might push it to ~4-5 tok/s. This is beneficial for latency.
* CPU (NEON) Fallback: If neither GPU path is viable (or if the model is very small where GPU overhead isn't worth it), llama.cpp on 8-core CPU with NEON acceleration will be used. We'll ensure the build has GGML_CPU_ACCELERATE=ON (which it is by default) to use NEON. We will still allow users to choose CPU-only mode in settings, as sometimes sustained GPU use may heat the device.
* Hexagon DSP Offload (QNN): For supporting helper models (embeddings, safety, etc.) on Snapdragon, we will integrate Qualcomm's Hexagon NPU via the Qualcomm Neural Processing SDK (QNN). There are a few routes: the simplest is using TensorFlow Lite's Hexagon delegate or ONNX Runtime's QNN Execution Provider. We will likely choose ONNX Runtime (ORT) because it supports QNN and gives flexibility with model formats. Our plan: convert the smaller models (e.g. a 100M param embedder or a 86M guard classifier) to ONNX, then use ORT built with QNN EP. We will statically link the needed QNN libs (provided by Qualcomm's SDK) into our native libs or bundle them as .so (noting license requirements). At runtime, if on a Qualcomm device with Hexagon, we initialize ORT with the QNN EP. The QNN EP will offload supported ops (which for common transformer-based models includes matmul, conv, etc.) to the Hexagon DSP. This should drastically speed up inference for those small models and save CPU cycles. For example, Qualcomm's AI Engine direct can run an 86M param model in tens of milliseconds if quantized[14]. We will quantize these models to int8 where possible to fit DSP constraints. If QNN is not available (e.g. on non-Qualcomm devices), ORT will silently fall back to CPU, so it's still functional. We will wrap this in a helper so the rest of the app just calls, say, GuardModel.check(text) and gets a result, without knowing if it ran on DSP or CPU.
* NNAPI fallback: Android's NNAPI can target various accelerators (DSP, NPU, GPU) generically. We may use NNAPI for some very small models or if QNN integration proves too complex. For instance, if we have a 5MB toxic-content classifier TFLite model, we can run it with NNAPI and trust the device to use whatever hardware available (DSP on Pixel, etc.). We note that Android's future is shifting to vendor-specific APIs (Gemini on Pixel, QNN on Qualcomm), so NNAPI might not yield the best performance. Still, having NNAPI as a backup ensures broad compatibility. We'll include NNAPI support in our TFLite interpreter for any TFLite-format models (e.g. we can try NNAPI for the Whisper encoder if it's 8-bit quantized).
Recommended Build Flags & Settings: We will maintain separate CMake builds: one for Snapdragon (OpenCL) and one for Mali (Vulkan), and include both .so in the APK (or a single .so that has both backends enabled - llama.cpp supports building with multiple backends simultaneously[15]). For example, we can compile llama.cpp with both GGML_OPENCL=ON and GGML_VULKAN=ON to cover both. At runtime, llama.cpp allows specifying device via --device flag (e.g. opencl:0 or vulkan:0)[16]. In JNI, we will use the C API (likely something like llama_context_params.n_gpu_layers or similar) - but since llama.cpp now supports multiple GPU types, we might see an API to choose backend. If not, we will set environment variables or call an internal selection function. We'll also ensure BUILD_SHARED_LIBS=OFF to link static and avoid dependency issues[17]. For Vulkan on Android, no extra .so is needed (since Vulkan is part of OS), but for OpenCL we include the ICD as mentioned. We'll use NDK r26+ which has up-to-date C++17 support needed by llama.cpp.
Memory & Model Size Considerations: We will provide guidance or automate model selection based on RAM. Rough rules: a 7B 4-bit requires ~3.5GB RAM (plus overhead for cache), so 7B models are suitable for devices with ≥6GB RAM. 13B models (even 4-bit ~7GB) are borderline on 8GB phones and not recommended for mid-range. The app's model manager will warn or restrict model downloads if the device has insufficient RAM. For example, on a 4GB RAM phone, we stick to 3B-4B models in 4-bit. We'll maintain a small JSON of recommended combos: e.g. 4GB - use 3B 4-bit, 6GB - up to 7B 4-bit, 8GB - 7B 5-bit or maybe 13B 4-bit if aggressive, 12GB+ - can try 13B 5-bit. We also consider VRAM (GPU memory) for GPU offload - but since we use mostly low-bit models, the entire model often fits in RAM and we offload layer-by-layer to GPU (llama.cpp streams layers). The KV cache memory can grow with context: we'll implement limits (like max tokens or use paging as below).
In summary, the app will dynamically choose the fastest config: Snapdragon phones will default to OpenCL acceleration for the LLM[5], and use Hexagon for helper models, whereas Exynos/Pixel will default to Vulkan for LLM and CPU for helpers (or NNAPI if available). All choices respect runtime checks and user overrides (an "Advanced" settings screen can list detected hardware and allow toggling GPU/DSP usage).
3. Speculative Decoding & KV Cache Management on Mobile
Speculative Decoding is an advanced technique to improve per-token latency by generating tokens in a "draft" manner and only occasionally consulting the full model[18]. We plan to prototype a two-model speculative decoder given the constraints of mobile:
* Draft & Verify Approach: We will include a smaller "draft" model (e.g. a 3B parameter distilled model) that can generate tokens quickly. For each decoding step, the process is:
* Use the draft model to predict the next N tokens autoregressively (e.g. draft 3 tokens ahead) very fast.
* Run the large model in parallel to verify those tokens. We can leverage batch inference on GPU: for example, create input sequences for the large model consisting of:
o the current prompt (to get the large model's prediction for token1),
o prompt + token1 (to get pred for token2),
o prompt + token1+token2 (for token3), etc., up to N[19][20]. We feed this as a batch to the big model's forward pass (Vulkan/OpenCL allow some batching, though we must be mindful of memory).
* Compare the big model's output distribution at each step to the draft token: essentially check if the big model would have produced the same token as the draft did[21]. If yes for all N tokens, we accept the whole chunk - thus producing N tokens in the time of one large-model pass (speedup).
* If a mismatch occurs at position j (e.g. big model's top token != draft token at that step), we rollback to that point: accept tokens up to j-1, and set the big model's state to the prompt + accepted tokens, then proceed normally (possibly even using draft model again for the next attempts).
In best case, the draft is correct for several tokens and we gain ~N× speed. Worst case, it guesses wrong immediately and we incur a bit of wasted work (big model did a batch for tokens that get thrown out). The key assumption is that the draft model is often correct for "easy" predictions (like common words, predictable continuations)[22]. Language modeling has a lot of redundancy and predictable syntax, which a smaller model can handle, leaving the large model to correct only at uncertain points.
* Medusa Multihead Approach: Alternatively, if we use a single model fine-tuned with multiple heads (Medusa), the model itself can output multiple tokens per step[23]. However, implementing Medusa requires having a specialized model (e.g. a Llama2 Medusa variant where extra decoder heads predict token2, token3, etc. in one forward). This is interesting but would require us to load a fine-tuned model with ~1.5× parameters (since extra heads) and likely isn't available for our smaller models yet. For now, we plan to stick to the two-model draft approach, which is more general.
* Recurrent Drafter (ReDrafter): We are aware of Apple's ReDrafter algorithm[24] which refines the speculative decoding by iteratively using the large model to guide the small model. Implementing full ReDrafter might be too heavy for our first versions, but we can take inspiration: e.g. periodically fine-tune the draft model on outputs the large model would produce, to keep them in sync. This is more of an offline training consideration. In code, our initial approach will be simpler: one draft attempt per token.
Implementation in llama.cpp: We will integrate this at a high level above llama.cpp's generation loop. For example, when the user prompts the system, we will initialize two contexts: ctx_big (large model) and ctx_draft (small model). Pseudocode for generation:
string prompt = ...;
auto state_big = llama_start(ctx_big, prompt);
auto state_draft = llama_start(ctx_draft, prompt);
while (!done) {
    // Draft N tokens using small model
    std::vector<int> draft_tokens;
    for (int i = 0; i < N; ++i) {
        int token = llama_predict(ctx_draft, state_draft);
        draft_tokens.push_back(token);
        llama_update_state(ctx_draft, token);
    }
    // Prepare batched inputs for big model
    std::vector<std::vector<int>> batch_inputs;
    batch_inputs.push_back({}); // base prompt (big model state already has it)
    for (int j = 1; j <= N; ++j) {
        batch_inputs.push_back(draft_tokens[0:j]); 
    }
    // Run big model for each sequence in batch (one forward pass if possible)
    auto batch_outputs = llama_predict_batch(ctx_big, state_big, batch_inputs);
    bool all_match = true;
    int match_count = 0;
    for (int j = 0; j < N; ++j) {
        int big_token = batch_outputs[j].token;  // predicted next token for sequence j
        if (big_token == draft_tokens[j]) {
            match_count++;
            // If first token matches, we can advance big model state one token
            llama_update_state(ctx_big, draft_tokens[j]);
        } else {
            all_match = false;
            break;
        }
    }
    if (all_match) {
        // Accept all N tokens
        appendTokensToOutput(draft_tokens);
    } else {
        // Accept tokens up to match_count, then have to generate next token by big model normally
        appendTokensToOutput(draft_tokens[0:match_count]);
        // Now generate next token with big model (since draft failed at match_count)
        int true_token = llama_predict(ctx_big, state_big);
        appendTokensToOutput({true_token});
        // Sync the draft model state with the big model state from here
        llama_set_state(ctx_draft, llama_get_state(ctx_big));
    }
    // loop continues until end-of-text or length limit
}
The above is conceptual - the real implementation must manage logits and states carefully. We'll need to use llama.cpp's low-level API to run a batch. If llama.cpp doesn't directly support parallel batch in one call, we might simulate it by sequential calls (less efficient). In the future, we might consider using the HuggingFace text-generation-inference library as reference, since it supports speculative decoding (Medusa and an N-gram mode) in its pipeline[25][26].
Feasibility on Mobile: The biggest concern is memory and compute overhead. Running two models means extra RAM - but our plan is to use a small drafter model (e.g. 3B parameters or even a 1.5B if available). A 3B 4-bit model is ~1.5GB, which might be too high combined with a 7B (~3.5GB) on an 8GB device. Instead, we might use an ultra-small model (e.g. 1-2B parameters distilled from Llama2 or a specialized "draft model"). Such a model in 4-bit might be ~0.5GB, which could fit. Alternatively, consider multi-round drafting: run the small model for a bunch of tokens offline (since it's fast) and then catch up the big model. We will experiment with N=1 or 2 first (predict 1 token ahead; essentially this becomes a simple speculative decode where the small model just proposes a token and big model confirms or rejects in real-time). Even that can give boost if the small model is right >50% of the time.
We will prototype this early (as it's high-risk complexity) on a flagship device to measure the speedup. If speculation yields, say, a 30-50% speed gain with minimal quality loss, we'll include it as an "Acceleration" toggle for high-end devices.
Paged KV-Cache: To manage long conversations without memory blow-ups, we will adapt vLLM's PagedAttention idea[27][28] to our context. Normally, llama.cpp allocates a contiguous KV cache for the maximum context (e.g. 4096 tokens). If we want to support longer context or multiple parallel chats, this becomes heavy. With a paged approach, we treat the KV memory as chunks (pages) that can be dynamically allocated and freed. For implementation: - We can modify llama.cpp's memory arena to allocate KV in fixed-size blocks (say 256-token blocks). We maintain a lookup table from position to block, rather than using array indices[29]. This means the attention mechanism will need to translate a key index via the table. The vLLM project did this with custom CUDA kernels; on mobile, we could simpler allocate each block as a separate ggml_tensor. - Eviction: We can evict oldest blocks if memory is full. For example, if the conversation exceeds, say, 8k tokens and the user is unlikely to refer back to the very earliest tokens, we could drop the earliest 1k tokens of context (and perhaps summarize them and prepend a summary as new context). This is a higher-level strategy - effectively semantic compression of old turns. Our design will include a configurable context limit (e.g. default 2048 tokens). If exceeded, the app can either refuse to continue until user clears or automatically summarize and trim.
* Sharing KV across generations: PagedAttention also allows reuse of KV for parallel generations (like generating multiple continuations for sampling)[28]. On-device, we might not often do parallel prompts (except maybe for multi-sample responses or for tools). But if we implement beam search or multiple candidates for, say, reranking, the KV sharing would help. We can consider it later.
In practice, to start, we will implement a simpler measure: limited context with graceful degradation. For instance, if the user has a long chat history reaching 3000 tokens and then asks a new question, the app might automatically summarize older messages to free up ~500 tokens space. This summary generation can happen when idle (since we have the local LLM). This is not exactly paged memory, but achieves a similar goal (memory bounded, conversation coherent via summary). In code, we track the conversation tokens; if >X, we perform summary and trim the conversation list.
KV offloading: Another idea is to offload older KV pages from GPU memory to CPU (system RAM) to save VRAM. vLLM did this on PC by using unified memory. On Android OpenCL/Vulkan, there's no built-in unified memory management for us, but we could manually copy some buffers to RAM if needed. This again is complex and likely unnecessary unless we push beyond 4k context on a mobile device. Initially, we'll target a max 4k context which fits in RAM for 7B models.
Flash Attention: llama.cpp doesn't yet have full flash-attention on mobile (except perhaps in CUDA). If needed, we could incorporate a flash-attention approach in Vulkan compute shaders to speed up long context attention. However, that's deep in the weeds; we note it as a possible future optimization if long contexts become key.
In short, our plan for KV-cache is to avoid unbounded growth. We'll set sensible limits and possibly implement a circular buffer or paging if llama.cpp adds support. We'll keep an eye on the llama.cpp project updates in case a paged KV is introduced natively (which we could adopt rather than reinventing). If we find references or open-source implementations, we'll leverage those.
4. On-Device Retrieval-Augmented Generation (RAG) Design
For on-device RAG, we need to vectorize documents and efficiently retrieve relevant chunks at query time - all offline.
Vector Store Options: We consider two main approaches: - SQLite + sqlite-vec: Using SQLite extended with vector search capabilities[2][30]. The sqlite-vec extension supports storing embedding vectors (as blobs) and querying via k-nearest neighbors with cosine or L2 distance, accelerated with NEON on device[31][32]. It's lightweight (no external dependencies beyond the extension) and keeps everything in a single .db. Great for our privacy-first goal and "small/medium" dataset sizes. It may not handle millions of vectors super-fast, but for a personal knowledge base (likely a few thousand notes or web pages), it's ideal: "for local, embedded applications, it's shockingly capable"[30]. Also, using SQLite means we can combine text filters or metadata conditions easily in SQL.
* ObjectBox Vector DB: ObjectBox provides a high-performance local database with a vector indexing feature (using HNSW algorithm)[33]. Its advantages: very fast approximate search (HNSW can handle millions of vectors in milliseconds)[34], and a nice Kotlin API for defining entities and queries (as shown with the City example using @HnswIndex annotation[35]). It's a more "batteries-included" solution (we don't manually manage indexing; it's automatic). However, adding ObjectBox increases app size (its native library ~1-2MB) and one must consider its license (ObjectBox is generally free to use, but closed-source). Given our scenario is an open-source app with moderate data, SQLite-vec is the preferred choice - it aligns with being lightweight and fully offline with no additional services. We will keep our vector store abstract so we could swap in ObjectBox later if needed (for example, a user with 100k embeddings might prefer the performance of HNSW indexing).
Schema Design (SQLite): - We will create a SQLite database (say iris_rag.db). Using sqlite-vec, define a virtual table for embeddings. For example:
CREATE VIRTUAL TABLE IF NOT EXISTS embeddings USING vec0(
    id TEXT PRIMARY KEY,
    embedding BLOB,          -- vector stored as blob (float32 array)
    meta TEXT                -- JSON or text for metadata (optional)
);
The vec0 module allows a query like SELECT id, distance FROM embeddings WHERE embedding MATCH $query_vec ORDER BY distance LIMIT 5; to get nearest neighbors[36]. We might also have a separate table for the actual text:
CREATE TABLE IF NOT EXISTS documents (
    id TEXT PRIMARY KEY,
    content TEXT,
    source TEXT,   -- e.g. "note", "pdf", "web", etc.
    timestamp DATETIME
);
The id would link the two (so typically the id could be a UUID or a path reference). We store full content in documents (or perhaps chunk content, see below).
* Chunking: Long documents are broken into chunks (e.g. 256 tokens each) before embedding. We can either treat each chunk as a separate entry in documents (with its own id) or store a chunk_index field. For simplicity, we can set id = docID_chunkIndex (like "doc1_0", "doc1_1") in the embeddings table. The metadata can include the source doc id and chunk index, so we can reconstruct or display properly.
* Embedding generation: We will likely use a small embedding model like nomic's text embedding model (if available in GGUF) or a MiniLM converted to GGML. For example, there are instructor models or E5 small models that could fit. The chosen model should produce, say, 384 or 768-dimensional embeddings. We'll quantize it (maybe 4-bit or 8-bit) to run fast on device. The RAG layer will call JNI to embed text: e.g. vector = Embeddings.embed(text), which uses llama.cpp with the embedding model loaded to output a vector. This is done for each chunk during ingestion.
* Distance Metric: We likely use cosine similarity for text embeddings (common for semantic search). sqlite-vec supports cosine distance natively (it actually provides a distance function that can do L2 or Cosine)[31]. We may store normalized vectors to simplify using L2 as equivalent to cosine.
Ingestion Workflow: 1. Data selection: The user chooses what to index - e.g. they toggle on "Index my notes" or select specific files. The app will iterate through selected data sources. For each document: 2. Chunking: Split the text. For plain text or markdown, we chunk by paragraphs or a sliding window of ~200 tokens (so each chunk is a self-contained semantic unit). For PDFs or images (with OCR via MediaPipe or Tesseract if we do OCR), we extract text then chunk. We'll likely ignore extremely short or irrelevant chunks (we can apply a stop-word filter or require some meaningful length). 3. Embedding: For each chunk, call the embedding model to get a vector. This happens in a background thread - possibly using a smaller thread pool separate from the main LLM so that we can still serve user queries without blocking (if the device can handle it). 4. Storage: Insert into SQLite in a transaction: add an entry in documents (if not already for that doc) and one in embeddings (with id "docX_chunkY", the vector, and metadata such as {"doc":"docX","chunk":Y,"text_preview":"first 50 chars..."}). We commit after a batch of inserts for efficiency. 5. Book-keeping: We record that these docs are indexed (so we don't duplicate later). Perhaps maintain a simple log or hash of content to detect changes.
We will implement ingestion as an opt-in process: maybe a "Index Now" button or automatic indexing when the device is charging (since embedding lots of text is CPU/GPU heavy and can drain battery). We also provide a "Delete All Embeddings" option: which simply wipes the SQLite tables (ensuring the user's original data remains but the index is gone).
Query Workflow (when user asks a question): 1. The user's query (plus possibly context of conversation) is fed to the embedding model to produce a query vector. 2. We perform a nearest neighbor search in the SQLite vector table: e.g. SELECT id, distance FROM embeddings WHERE embedding MATCH 'X' LIMIT 5; - the MATCH operation uses the sqlite-vec KNN virtual table under the hood[36]. We get e.g. top 5 chunks. This operation is very fast (a few milliseconds) for a few thousand vectors, thanks to NEON acceleration in sqlite-vec. 3. We retrieve the corresponding texts of those chunks from documents table (join on id or use a separate query for each id). 4. (Optional) Rerank: For better accuracy, we could run a small cross-attention model or even use the main LLM to rerank which chunks are truly most relevant. A cheap approach is to concatenate the question with each retrieved chunk and ask a small model for a relevance score. However, given device constraints, we might skip heavy reranking and rely on the embedding distances or a simple heuristic (like boost more recent notes). 5. We then construct the augmented prompt for the LLM: e.g. "You are a helpful assistant. Use the following retrieved information to answer the question.\n\nRelevant Info 1: <chunk text>\nRelevant Info 2: <chunk text>\n\nQuestion: <user question>\nAnswer:" - basically a prefix that provides the retrieved texts. We ensure each chunk is clearly demarcated, maybe by number or by source citation. 6. The LLM generates an answer, presumably using the provided info. We can also instruct it to cite or refer to the info by number (for user transparency).
To keep latency low: - The embedding of the query is quick (embedding model is small; should be <100ms on flagship, maybe 200-300ms on midrange). - Vector search in SQLite is on the order of milliseconds (and can be done on a background thread). - The main cost is still the LLM generating the answer (multiple seconds). This is fine as it's the core task. - We will pipeline where possible: e.g. start the LLM thinking with the question itself, and slightly delay injecting the retrieved info until it's ready. However, that's advanced; more straightforward is to always wait for retrieval, then feed full prompt. The slight overhead of 0.2s is negligible compared to, say, a 5s generation.
Preventing Index Bloat: - We will not index binary data or extremely large text by default. The user will specifically opt-in sources. We can also put a cap like "max 10,000 chunks" overall, and inform the user if reached. - Perhaps implement basic deduplication: if two notes are very similar (or one is a copy), we might avoid double-indexing. This could be done by hashing chunks or by skipping indexing of identical text. - Provide UI to manage indexed data: e.g. a list of sources indexed and how many items. If user deletes a note or toggles off a source, we should offer to remove those entries from DB (we can listen for deletions if possible or require manual refresh). - Periodic maintenance: maybe compress the DB or re-embed if we upgrade the embedding model (rare).
Updates/Deletions: - We need a way to update the index when user data changes. For simplicity, an "Index All" that reindexes everything periodically could be used. But that might duplicate entries. Instead, we'll maintain, for each item, a stored hash or timestamp of last index. For example, for each note, store in a small table indexed_items(item_id, last_index_time, item_hash). When re-indexing, compare current content hash; if it changed, remove old embeddings for that doc (delete by doc id prefix) and then index fresh. - If user deletes a source (say removes a PDF file), we should remove those entries too. We'll need to identify them by doc ID or metadata. This can be done by keeping a mapping of file path to doc ID.
Latency considerations: - The RAG augmentation will increase the prompt length (because we attach chunks). This can slow down the LLM (longer prompt to process). To mitigate, we limit to top ~3 chunks or a fixed token budget (e.g. max 500 tokens of retrieval). Also, on slower devices, user can disable RAG if they prefer pure LLM responses. - We might also optimize by summarizing or compressing the chunks before inserting into prompt. Possibly use a smaller model to compress a long chunk into a sentence if needed.
Privacy and Controls: - All embedding and search is on-device, so user data stays local. We will make this clear in UI (e.g. a label "Your data is processed locally"). We also give user fine control: for example, separate toggles for "Index my SMS", "Index my Contacts notes", etc. Initially, we might just allow file-based import (like "Select PDFs to index") and a general notes folder. - A "Pause indexing" option could be given if the user is low on battery or doesn't want background activity.
In summary, our RAG pipeline is: local embeddings + SQLite-vec by default, which is well-suited for edge AI on mobile[30]. For a future scenario where someone has a very large personal dataset, we can consider switching to an approximate index (ObjectBox's HNSW which handles millions with ms latency[34]). But likely, the bottleneck will be the LLM itself far before the retrieval becomes an issue for personal scale data.
5. Multimodal & Speech Integration
Speech Recognition (ASR): We'll incorporate offline ASR so users can speak to the assistant: - whisper.cpp: We will compile the whisper.cpp library (also based on ggml) for Android. The integration is straightforward: whisper.cpp provides functions to load a model and transcribe audio. We'll add a JNI interface, e.g. transcribe(byte[] pcmData) that returns the text. Since whisper models are large (even tiny model is ~30MB, base ~140MB, large ~1.5GB), we'll include by default a small model (maybe Whisper tiny or base). Possibly we can use Whisper small or a distilled variant on high-end devices. There are distilled Whisper models (like Whisper-medium distilled to ~40% size) that can run faster. There's also Faster-Whisper (CTranslate2) which is highly optimized (int8, multi-threaded). We can consider shipping CTranslate2's libs and a small model if it yields better speed. For example, Faster-Whisper base can transcribe real-time on a phone using 4 threads, whereas whisper.cpp base might be slightly slower. The user explicitly mentioned bundling CTranslate2 for performance - we will evaluate it: it would mean adding ~10MB of C++ libs but could double ASR speed.
* Real-time performance: Our goal is near real-time transcription for short utterances (<10s). On a mid-range device, Whisper tiny (39M) can transcribe ~1x or faster than real-time. On flagship, Whisper base (74M) might do realtime. We can default to tiny model for mid-range and base for flagship, with an option to download a larger model if user wants better accuracy. Distilled Whisper models (if available) could improve speed ~2x for same size.
* Integration flow: When user taps the mic, we start audio recording (using Android's AudioRecord). Once the user stops (or with VAD, detect silence), we take the PCM audio buffer (16k mono) and pass to transcribe(). The result text is then treated as the user's query. We might also do streaming ASR for longer dictation (whisper.cpp supports partial results callback), but initial version can be one-shot for simplicity.
* We will also include Text-to-Speech (TTS) output for the assistant's responses. Since TTS models offline are big, we may use Android's built-in TTS (which uses device voices and is efficient). It's not fully offline on some devices (Google's engine sometimes does processing in cloud unless download voice packs). As an alternative, Mozilla's Larynx or Coqui TTS could be integrated on-device, but that's heavy and beyond scope for now. Using system TTS with an appropriate voice (maybe the user can choose their phone's default voice) will suffice. We just ensure to wrap the LLM's answer and feed to TTS API for speaking.
Vision Support: - We aim to let the assistant take images (from camera or gallery) as input. Full image-to-text models (like BLIP or MiniGPT-4) are too heavy (billions of params). Instead, we adopt a modular approach: - Use MediaPipe (or Google ML Kit) for specific perception tasks: - For example, face detection (MediaPipe Face Detection), object detection (MediaPipe has a general object detector or we can use a small YOLO model), image labeling, OCR (ML Kit Text Recognition). - These models are optimized for mobile (running on GPU/NNAPI). - After getting structured info (like "Detected objects: cat, couch; The text in image: 'Hello from Paris'"), we feed that as part of the LLM prompt. - For example: User says: "Describe this picture", System does: detect objects -> "Image contains a cat on a couch.", detect scene -> maybe use a small MobileNet classifier for scene -> "Indoor living room.", OCR -> any text found. Then we formulate: "The user provided an image. Here is what I see: A cat sitting on a couch in a living room. There is a window in the background. The user wants a description." and give that to LLM. The LLM then can produce a more fluent description or answer. - This essentially uses the LLM as the reasoning layer on top of perceptions from smaller models - a form of multimodal RAG (retrieve facts from the image via detection models, then generate). - On devices that can handle it, we keep an eye on llama.cpp's multimodal support. For instance, models like LLaVA or Qwen-VL (vision-LLMs) can be run if we load the vision projector model (the mmproj file) and the LLM. This would allow end-to-end image input. If a smaller multimodal model (like MobileVLM or Gemma-3 1B vision) becomes available in GGUF, we can experiment. In the llama.cpp libmtmd, they list some vision-capable models like Gemma 3 (2B vision-enabled)[37]. A 2B model with images might be feasible on a high-end phone (with quantization). - In Phase 3 (later), we might integrate one of those directly: e.g. load the 2B vision model when user tries an image query and get the response. But initially, the perception-pipeline approach is lower hanging fruit.
* Gemini Nano / AICore Integration: Google's new AICore (on Pixel 8) with Gemini models is intriguing. If Google provides an API for third-party apps to use on-device generation or summarization, we will integrate it as an optional accelerator for Pixel users. For example, if there's an Android 15 API that says AiServices.summarizeCall(audio) or some On-Device Personalization API with LLM capabilities, we'll utilize it for those specific tasks (call transcript summarization, etc.). Until then, Pixel's AICore is largely proprietary. We can leverage it indirectly via NNAPI if Google exposes the Gemini model through it (not likely for now). So we consider it out-of-scope for initial phases except to design our architecture flexibly (e.g. have an interface for "summarize text" which can be implemented by our local model or by a Pixel-specific call under the hood).
* Example use-cases:
* Voice conversation: User taps mic, asks "What's the weather like in this photo?" while showing a picture. We capture audio -> text ("What's the weather...photo"), capture image -> detect it's outdoors sunny. The system then asks LLM: "User asks: What's the weather like in this photo? The photo appears to show a sunny day with clear skies." The LLM responds "It looks bright and sunny." The app speaks it out via TTS.
* Live camera: Possibly we could have a mode where user points camera and asks "What do you see?" - we run MediaPipe in real-time (if feasible) and speak out results via LLM. This is ambitious; probably we'll focus on static images first.
Model sizes for devices: - Mid-range: Use Whisper tiny (or base if moderate), smaller detect models (MediaPipe is optimized even for mid-range). - Flagship: Can use Whisper small or medium (with multi-thread or GPU via Vulkan - we might try offloading Whisper to GPU via OpenCL too, as whisper.cpp has OpenCL support). - The vision models used via MediaPipe are lightweight (e.g. face detector ~1MB, object detector few MBs). Those run real-time on flagship (and ~10-15 FPS on mid-range possibly). Because we don't need video stream analysis, just one image at a time, that's fine.
CTranslate2 vs ggml for Whisper: - If using Faster-Whisper (CTranslate2) for better speed, we'll need to bundle its binaries for each ABIs. It can use OpenMP for multithreading, and possibly GPU via OpenCL (CT2 can use OpenCL on Intel/AMD, not sure about Adreno). There is also a Qualcomm-specific acceleration for Transformer via QNN for audio? Unlikely, we'll use CPU threads. - We should compare: if whisper.cpp transcribes 30s audio in 20s on device and CT2 does it in 10s, then CT2 is worth including. If difference is small, we stick to whisper.cpp to avoid two engines.
MediaPipe integration: - We can use Google's Task Library which provides ready-made solutions for detection, face mesh, etc., with GPU acceleration. They have AAR packages we can add. Alternatively, use TFLite with appropriate models. MediaPipe is easier as it does a lot of heavy lifting (like converting camera frames to input). - We will likely do a simple approach: user selects an image from gallery (or we capture a photo), we run detection on a background thread, then pass results to LLM. So we don't necessarily need the full MediaPipe graph running continuously.
Pros/Cons of relying on cloud vs local: - We explicitly stay offline for all these; we won't use Google Lens or cloud speech, to maintain privacy and offline capability. The downside is possibly lower accuracy or slower performance. But that's the trade-off we accept for privacy. We can highlight this to users ("All vision and voice analysis is on-device; no data leaves your phone").
In summary, speech and vision broaden the assistant's capabilities: voice input/output will make it hands-free, and basic image understanding addresses common user requests. We focus on efficient, small models to achieve this. Over time, as device hardware improves, we can slide in more advanced multimodal models (like Qwen-VL or open-sourced Gemini Nano if that appears).
6. Tool Calling and Deep Android Integration
To enable the assistant to perform actions on the device, we will implement a tool invocation system integrated with Android's intents and APIs, all while keeping the user in control.
Tool Schema: We'll define a set of tools with names, functions, and parameters. For example: - Tool: "create_calendar_event" - Function: Create an event in calendar - Params: { "title": string, "datetime": string, "duration_mins": int, "description": string? } - Tool: "send_sms" - Send an SMS message - Params: { "to": string (phone number), "message": string } - Tool: "open_app" - Launch an app by name - Params: { "app_name": string } - Tool: "set_alarm" - Set an alarm clock - Params: { "time": string or hour/min fields, "message": string? } - Tool: "call_tasker" - Trigger a Tasker task or MacroDroid macro - Params: { "task_name": string }
We might encode these in a JSON or YAML that the LLM can read (in a system prompt we enumerate available tools and their usage). Llama.cpp supports an OpenAI-like function calling format now, where the model can output a JSON with function and args if properly prompted. We will leverage that: prompt the model in system message with a schema like:
[ 
  { "name": "create_calendar_event", "description": "Create a calendar event on the device", 
    "parameters": { "type": "object", "properties": { "title": {"type":"string"}, "datetime": {"type":"string"},"duration_mins": {"type":"integer"},"description": {"type":"string"} }, "required": ["title","datetime"] } },
  ... 
]
Then if user asks "Schedule a meeting tomorrow at 10am called Project Sync", the model ideally outputs a function call like:
{ "tool": "create_calendar_event", "args": { "title": "Project Sync", "datetime": "2025-11-06T10:00", "duration_mins": 60 } }
We will parse this JSON in our agent layer.
Mapping to Android Actions: - create_calendar_event → We construct an intent:
val intent = Intent(Intent.ACTION_INSERT).setData(CalendarContract.Events.CONTENT_URI)
intent.putExtra(CalendarContract.Events.TITLE, title)
intent.putExtra(CalendarContract.EXTRA_EVENT_BEGIN_TIME, datetimeInMillis)
intent.putExtra(CalendarContract.EXTRA_EVENT_END_TIME, datetimeInMillis + duration_mins*60000)
// etc.
This intent opens the Calendar app's insert screen with details filled. We might prefer direct insertion via ContentResolver for fully automated action, but that requires WRITE_CALENDAR permission and can be intrusive. Better UX is to launch the calendar entry for the user to confirm/save (less "magical" but safer). - send_sms → Use SmsManager if we want to send without UI (requires SEND_SMS permission) or use an intent:
val intent = Intent(Intent.ACTION_SENDTO, Uri.parse("smsto:$number"))
intent.putExtra("sms_body", message)
This brings up the SMS app with the message (user can hit send). Given Play Store policies, auto-sending SMS might be restricted, so likely we use the intent approach. - open_app → Use PackageManager to find app by name and then startActivity(launchIntent). We'll have a mapping or simple search: for "YouTube", find com.google.android.youtube package, etc. If ambiguous, the assistant should ask for clarification (we can also present a chooser). - set_alarm → Use AlarmClock intent:
val i = Intent(AlarmClock.ACTION_SET_ALARM)
    .putExtra(AlarmClock.EXTRA_HOUR, hour)
    .putExtra(AlarmClock.EXTRA_MINUTES, minute)
    .putExtra(AlarmClock.EXTRA_MESSAGE, message)
This opens Clock app's alarm set UI (or sets directly if we add EXTRA_SKIP_UI). - call_tasker → Tasker has a secondary app or we can use broadcast intents that Tasker can receive. Tasker can listen for net.dinglisch.android.tasker.ACTION_TASK with an extra for task name (if Tasker is installed). MacroDroid similarly can be triggered by broadcasting an intent or calling a specific URL scheme. We'll document how to integrate those (likely an advanced user feature requiring them to configure the integration).
We will maintain these tool definitions in a structured way so that both the prompt and the parsing code use the same spec (maybe define in JSON and then load into both prompt and a Kotlin data class).
Safe Execution Patterns: Every tool action that can have side effects will require user confirmation before execution: - When the LLM produces a tool JSON, we pause the LLM output and show a confirmation UI: e.g. a bottom-sheet dialog: "Assistant wants to: Send SMS to 1234567890 with text 'See you at 5!'" and options [Allow] [Decline]. - Only if the user taps Allow do we proceed to perform the action. If Decline, we either drop it or pass a message back to LLM like an error (e.g. "Tool execution declined by user" which the LLM can then respond to). - We log the action in an internal audit log (maybe just keep last N actions in memory or a file), so the user can review what the assistant has done recently.
Permissions: We need to handle runtime permissions for SMS, Calendar, etc. For instance, if the user invokes send_sms for first time, we must request Manifest.permission.SEND_SMS before actually sending. Same for reading contacts if we implement any contact lookup. We'll ensure to check and request appropriate permission as needed, and handle the case where user denies (inform them the action can't be completed due to lack of permission).
App Shortcuts Integration: Android supports static and dynamic shortcuts (displayed when long-pressing app icon or via Assistant). We can use this to surface most-used or suggested actions: - As the user uses tool commands, we notice patterns. Suppose the user often says "Navigate home" and we map it to launching Maps with home address. We could create a dynamic shortcut with ID "navigate_home" and use ShortcutManager:
val shortcut = ShortcutInfo.Builder(context, "nav_home")
    .setShortLabel("Navigate Home")
    .setIntent(Intent(Intent.ACTION_VIEW, Uri.parse("google.navigation:q=Home Address")))
    .build()
shortcutManager.pushDynamicShortcut(shortcut)
- Similarly, if user frequently adds reminders, we add a shortcut "Add Reminder" that deep-links into our app's function (or directly to an Android intent if appropriate). - These shortcuts can also be suggested to Google Assistant (there's an API to publish shortcuts which Assistant can invoke by voice). So the user could use the system Assistant to trigger them, which indirectly triggers our app's action - effectively enabling voice commands offline via our app, interestingly. - We will limit the number of dynamic shortcuts (Android allows up to 5-10) to the most useful ones and update based on usage.
Integration Example: User says: "Remind me to call Alice at 9 PM". - LLM sees this, decides to use a tool: outputs JSON for create_calendar_event or maybe set_alarm (depending how we prompt it to categorize reminders). - The app parses this. Suppose it's set_alarm with time=21:00, message="Call Alice". The app pops confirmation: "Set an alarm at 9:00 PM labeled 'Call Alice'?". - User taps Allow. The app calls the AlarmClock intent to set it. The LLM is then given a success response (or we simply inform the user "Alright, I set an alarm." in chat). - Meanwhile, we add a dynamic shortcut for "Call Alice reminder" or generally for "Add Alarm/Reminder" because user used it.
Error Handling: If a tool fails (e.g. trying to open an app name that doesn't exist), we catch the exception and feed an error message to the LLM's context (like a function result). Then LLM can say "I'm sorry, I couldn't find that app.". We should design the prompt to handle tool responses. Perhaps we do a two-step: model outputs function call, we execute, then we feed the function's result back into the model (continuing generation). This is how OpenAI function calling works (they let model call function, get result, then model continues). We can emulate that by capturing the partial output, doing the action, then calling llama.cpp again with a system message containing the result, and let it continue the answer.
However, continuous LLM re-invocation is heavy. We might instead handle simple cases in code (for a known failure, just have the assistant's next message prepared). For MVP, we might have the LLM output the final answer directly including any tool effect description, to avoid multi-round. This requires the model to be very sure the action succeeded. Alternatively, we do a short second inference for final answer after executing.
Security: We will restrict tools to those we explicitly define. The LLM will be instructed that it can only call those functions (not arbitrary code). And the app will parse only known tool names. This prevents the model from doing something dangerous outside allowed scope. Additionally, user confirmation ensures nothing happens without consent. We also consider misuse: e.g. model tries to send SMS to a random number with user's private info - the user sees it and can deny.
One challenge is if the model is jailbroken, could it output some hidden command to exploit our parser. But since we strictly parse JSON for known keys, anything not matching the schema will be treated as normal text output (so it can't break out of the sandbox and run code, as long as our parsing is strict and not, say, using eval()).
Extensibility: We'll make it easy to add new tools. For instance, in the future, integrate smart home controls: define tool turn_on_device with params device_name. The architecture remains same. This opens up possibilities like local network API calls (e.g. if user has Home Assistant or we can use Android's built-in IR or IoT controls via intents if any).
So the tool/agent layer essentially acts as a local, permissioned gateway between natural language and Android system functions. By structuring it with clear confirmation and logging, we aim to achieve powerful integration without compromising user trust or safety.
7. On-Device Safety and Guardrails
Even offline, it's important the assistant behaves appropriately. Our safety approach will be multi-tier:
* Prompt Filtering (Pre-check): Before the main LLM sees the user's prompt, we run a guard model to detect problematic inputs:
* For jailbreaks or prompt injections, we use Llama Prompt Guard v2 (86M)[38]. This model classifies if the user is trying to get the AI to ignore instructions or produce disallowed content. If it flags "malicious" (i.e. likely a jailbreak attempt) with high confidence, we can take action: possibly refuse upfront or insert an automated system prompt like "(The assistant notices an attempt to override its safety)" to bias the LLM. We may start with a simpler approach: if detected injection, we tell the user "I'm sorry, I cannot comply with that request." and not even query the LLM. This is to prevent known jailbreak phrases from succeeding. The false positive rate of the guard model should be low due to its training[39], but we'll allow an override in developer mode.
* For content categories (violence, hate, sexual, self-harm, etc.), we can use a smaller classifier (e.g. OpenAI released some smaller classifiers, or use a distilled RoBERTa). However, Meta also has a Llama-2-7b-chat safety classifier, but 7B is too large. Instead, we might use something like OpenAI's content filter model (Harassment, Hate) if open-sourced, or rely on rules for obvious cases. Initially, we might implement a simple keyword-based filter for extreme content as a backstop, then introduce a learned model as we find one small enough (perhaps a 100M parameter multi-label classifier for those categories - we could fine-tune one ourselves on public datasets and quantize it).
If a user prompt is clearly disallowed (e.g. "How do I make a bomb?"), the pre-check will intercept and we can directly respond with a refusal ("I'm sorry, I can't help with that request.") without hitting the LLM.
* On-the-fly Prompt Adjustment: For certain queries that are borderline, we may choose to inject a safety system message to the LLM. For example, if the user asks something medical ("I am feeling very depressed..."), we let it through to LLM but we add a system note: "[The user might be seeking medical or self-harm advice; respond with care and encourage seeking help]". This kind of templated insertion can guide the model's response to be safer. We maintain a set of guidelines triggered by categories.
* Post-Generation Filtering: After the LLM produces a response, we run a safety classifier on the output. For instance, a small toxicity/hate speech model (like a distilled Detoxify model ~50M) could check the LLM's answer. If it contains high probability of disallowed content, we will intervene. Options:
* Mask or Modify: We could replace the offending parts with "[filtered]" or ask the LLM to rephrase. Since we're offline, we have full control: we can even loop back to the LLM with "Your last answer may be violating the policy, please rephrase it in a safer manner." This would require a second pass generation. This might be too slow to do regularly. A simpler approach: not show the user the original bad output, and instead show a generic apology.
* Warning: If minor (like a borderline joke), perhaps just warn the user that content might be offensive, depending on their safety settings.
* User Safety Settings: We will include a section in settings to let the user configure the level:
* Default: Safe mode on (no extreme content, avoid profanity).
* "Creative" mode: a bit more lenient (allow mild profanity or edgy humor).
* "Unfiltered" mode: we can allow the user to turn off most filters except truly illegal content. If they do this, we show a warning ("Disabling safety may result in offensive or inaccurate content. Proceed?"). This is for power users who want raw model behavior. But even then, the tool/intent actions will remain protected (we won't ever allow the model to, say, wipe the user's phone or send mass texts, because that's controlled by our confirmation layer, not by the model's output text).
* Model Choices for Safety: We mentioned Prompt Guard 86M for injection detection. It's multilingual and tailored for that[40]. We will quantize it (8-bit or 4-bit) and run on device (perhaps via the QNN/Hexam if possible to speed it up). For general content, one promising model is the OpenAI moderation model (there is a 2022 open-source equivalent around 300M, but that's bigger). There's also a recent 200M "Harmless" classifier (we will search for one). Alternatively, we can fine-tune a smaller LLM (like a 1-2B) on a safety classification task, but that may be too much work. In the interim, a heuristic approach (word lists + simple logic) might catch obvious cases.
* Policy Framework: We will define categories similar to OpenAI's: Hate, Harassment, Self-harm, Sexual (with minors separation), Violence, Illicit behavior, etc. For each, decide if content is disallowed or just needs caution. For example:
* Disallowed: sexual content involving minors (hard block), advice on violent wrongdoing, etc.
* Caution: user expressing suicidal thoughts (not disallowed to discuss, but we respond with empathy and encourage help, rather than giving instructions).
* Harassment/hate: We likely prevent the assistant from using slurs or hate even if user asks. If user inputs hate speech, the assistant should refuse or respond with a reprimand depending on setting.
* We will include a brief User Guide in-app that says the assistant might refuse requests that violate certain guidelines (this aligns with Play Store policy on AI content[41][42]).
* False Positives/Negatives: On-device models won't be perfect. If a user's harmless query gets flagged (false positive), they should have a way to know and override. Perhaps if our guard model blocks something, we can present: "Your query was flagged as potentially unsafe. If you believe this is a mistake, you can force the assistant to proceed." in developer mode. Conversely, if something slips through (false negative), ultimately it's an offline app, so the user is only possibly offending themselves. But if the output is e.g. extremely harmful advice, that's a concern. We mitigate that by having a reasonably instruction-tuned model and our filters.
* Small "Guard" Model Performance: The advantage of something like Llama Guard 86M is it's fast - 86M 4-bit is ~40MB, it can run in tens of milliseconds on CPU (especially if int8 quant). So it won't add noticeable latency. Running it on each prompt and each response is fine.
* Memory for guard model: We might keep it loaded alongside the main model to avoid loading every time. 86M is not too bad; if memory is tight, we could unload it when not needed or even run it on the NPU via QNN to free CPU. Or as documentation suggests, there's also a 22M version (with DeBERTa backend)[43]. The 22M would be even lighter (~90ms on mobile likely). We might test its accuracy versus 86M. If it's close, use 22M for speed.
* Output Moderation vs. Input: Play Store requires moderation of user-generated content as well (if users could generate images, etc.). In our case, user is inputting text or images. We should ensure if the user shares an image, we don't inadvertently output something violating privacy or so. Hard to moderate images offline beyond analyzing them - we do have the vision pipeline, which might detect if an image is inappropriate (like certain NSFW detection model could be added - there are lightweight NSFW detectors we could run). Possibly we skip NSFW image detection for now, focusing on text only.
* Transparency: We will be transparent with the user when we filter. Instead of silently blocking, we'll provide a response like "I'm sorry, I can't continue with that request." maybe with a brief reason if appropriate ("That content isn't allowed."). We avoid being too specific (which could teach how to evade).
To summarize, our on-device safety system uses local classifiers to guard the main model at input and output[40]. It forms a first line of defense that is fast and offline. Combined with the tool confirmation and user settings, this ensures the assistant remains helpful but within acceptable boundaries. And since everything is local, even the safety filters don't send data out - it's all contained, aligning with our privacy stance.
8. Battery, Thermal, and Play Store Compliance
Designing for mobile means being mindful of battery drain, device heat, and obeying Google's app policies.
Battery & Thermal Management: - Adaptive Load Shedding: We will utilize Android's ADPF (Android Dynamic Performance Framework) APIs to monitor thermal and battery state. Specifically, the Thermal API provides a ThermalStatus or headroom info (on Android 14+) indicating if the device is nominal, warm, or throttling[44]. We'll register a listener: if the device hits a high thermal state (about to throttle), we can proactively reduce LLM load. Strategies: - Reduce the thread count for generation (fewer cores = less heat). - Insert artificial delays between tokens to slow down and cool off (user might prefer a slower response over a force-close due to overheat). - If a very large model is loaded and causing heavy sustained load, we might even auto-switch to a smaller model mid-conversation ("Switching to a lighter model due to thermal constraints" - though that's jarring, better to handle next conversation). - ADPF also has CpuHeadroomParams and GpuHeadroomParams that give an estimate of how much more work we can do without jank[45]. We can tie the context length or max tokens to this: e.g. if headroom is low (meaning we're pushing limits), we can temporarily cap the max tokens the model will generate (cut off long rambling answers).
* Foreground Service & Task Scheduling: Long-running processing (like generating a 2 minute answer, or continuously listening for voice commands) should ideally run in a Foreground Service to avoid Android killing the app. We will implement a Foreground Service when the assistant is "active" but not in the visible UI (e.g. if screen is off and user issued a voice query via a hotword). The service will show a notification "Assistant is running... tap to open" to comply with background execution rules. We will carefully stop this service when done to not hog resources. If the user is in the app (UI visible), we don't need a foreground service because the app is in foreground.
* Job Scheduling: For background tasks like periodic indexing or model updates, use WorkManager with appropriate constraints (e.g. only run indexing when device is charging + idle, unless user explicitly triggers).
* Thermal Tuning: We will integrate with Game Mode / Perf Hint APIs to hint the system that our app can handle lower performance if needed. Android 14's PerformanceHintManager can allow us to cluster work into "intervals" - but that's more for frame rates. Instead, we might use simple things:
* Lower LLM generation speed when battery is low (e.g. if battery < 15%, maybe automatically use a smaller model or cut context to save compute).
* Provide a user option "Battery saver mode" which would do things like limit to 4 threads max, or use the smallest model even if bigger available, etc.
* Profiling Tools: During development, we'll use Android GPU Inspector and Perfetto to profile GPU and CPU usage while generating tokens[46]. We'll identify hotspots (e.g. if Vulkan compute kernels are fully loading the GPU for a long burst, see how it correlates with frequency throttling). Using these, we might adjust some parameters:
* For example, if we see that on a certain device, using 6 threads on CPU yields throttling after 30s, whereas 4 threads yields steadier performance, we might choose a default thread count based on device.
* We will document recommended settings like "On Pixel 6, enabling Vulkan might throttle, consider using 75% performance mode."
* Context Limits for Thermal: A novel idea is adjusting the length of responses in high thermal situations. If the phone is hot, we maybe shorten answers to wrap up sooner, thus stopping heavy compute. The assistant can say "(Device is getting warm, I will keep it brief)... [short answer]". Possibly not needed if other controls suffice, but it's an option.
* User Feedback for Heat: We can indicate in the UI if the app is running in a throttled state ("Device thermal limit reached, performance reduced") to be transparent.
Google Play Compliance: We will ensure the app follows all relevant policies, especially the new ones for AI: - AI-Generated Content Policy: Google requires that generative AI apps have proper moderation and do not produce disallowed content[41][42]. Our safety layer addresses this by filtering prompts and responses. We will explicitly test scenarios to ensure it doesn't output egregiously disallowed content (like hate or dangerous instructions). We'll document this in our app's metadata as needed for review. - The policy also suggests incorporating user feedback: we will have a "Report" button on each assistant response. If the user finds it offensive or wrong, they can flag it (this feedback can be stored locally or emailed to us for improvement since no server). This shows we allow user to help correct issues[47]. - Privacy: Since we handle user data (notes, etc.), we must have a privacy policy stating clearly that data never leaves device. We'll include that. Also mention no third-party has access (unless user chooses to export something). - No Personal or Sensitive Data misuse: The app will not send any user data off device, so we comply. If we use Crash reporting, we have to be careful to not log conversation content. - Restricted permissions: Sending SMS, reading contacts, etc. are sensitive permissions. Google will review why we need them. We will justify that in the app listing ("Allows the AI assistant to send messages or add calendar events on your behalf, with your confirmation."). We'll abide by SMS/Call Log policy (if we let it send SMS, we must declare and possibly they might reject if not core functionality. But since it's literally part of assistant core, it should be acceptable with a proper prompt). - No financial advice or medical advice disclaimers: If our assistant can produce info in those domains, we should include disclaimers. Likely, we add in our about or first-run message: "This AI is not a medical or financial professional. It may make errors. Do not rely on it for critical decisions." This is often required to avoid liability and I believe Play might require it if the app could be interpreted as giving such advice. - Age rating: We will mark the app appropriately (likely "Teen" at most) since we do have a safety filter. If we allow uncensored mode, we might need a higher rating or ensure it's opt-in and perhaps even age-gated. Probably better to keep default safe and label 12+ or 16+ due to potential mild language. - Intents and Accessibility: We must ensure the use of Android intents for user actions is not abused in background. Because we always confirm with user, we should be fine (no accidental pocket-dialing etc.). - Foreground service rules: Android 14 has restricted starting certain foreground services from background. Our flows will start them from UI or use schedule exact alarm if needed for voice hotword (which might not be implemented yet - likely user will press a button for voice, which is fine).
Thermal API usage example: We'll use ActivityManager.getThermalStatus() or the newer PerfManager API. If it returns critical (e.g. >= TROTTLING_SEVERE), we might pause the LLM generation with a message: "I'm going to stop here as your device is overheating." This is a last resort.
Testing & Tuning: We will test on various devices: - a mid-range (SD 700 series or older Exynos) to see how quickly battery drains and how hot it gets for a 2 minute conversation. - a flagship (SD 8 Gen 2) to ensure we hit performance but not meltdown. We'll look at Battery Historian logs to ensure we aren't keeping device awake unnecessarily. For instance, after a conversation, the app should release wakelocks and let device doze if possible.
Play Store Listing Compliance: We will explicitly mention the offline nature: "All AI processing is on-device. No data is collected." This should satisfy privacy requirements. Also mention it's experimental and not always accurate (their policies expect us to inform users that AI content might be incorrect or biased). Possibly include a phrase "Results may be inaccurate or offensive, we employ filters but please use responsibly."
Given that Google is now more accepting of AI apps but with oversight, our comprehensive safety will help in review. We'll prepare documentation for them if needed, explaining our safety measures (the Meta guard, etc.).
In sum, we'll make the assistant adapt to device conditions (not kill your battery or phone), and adapt to Google's rules (not produce disallowed content, proper user transparency). This dual adaptation ensures both a good user experience and a successful app submission.
9. Code Examples and Reference Integration
To illustrate parts of the plan, here are some code-level examples and relevant references:
* Building llama.cpp for Android GPU: Our CMake for llama.cpp might look like:
  cmake -B build-android -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake \
      -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=29 \
      -DGGML_OPENCL=ON -DGGML_VULKAN=ON -DGGML_OPENCL_USE_ADRENO_KERNELS=ON \
      -DBUILD_SHARED_LIBS=OFF .
cmake --build build-android --config Release
  This enables both OpenCL and Vulkan in one library[16]. At runtime we call llama_backend_init(device_type) based on detection (pseudocode; actual API might differ) to choose OpenCL for Adreno devices[48].
* OpenCL on Adreno performance: Qualcomm's blog notes that the OpenCL backend in llama.cpp is optimized for Snapdragon 8 Gen1/Gen2 etc., and supports popular models and quantizations[10]. They specifically mention using Q4_0 pure quant for best speed[7].
* Speculative Decoding Reference: The HuggingFace TGI documentation explains speculation: "generate tokens before large model runs, only check if those tokens were valid... 2-3x faster inference"[18]. This concept underpins our draft model approach.
* SQLite Vector Search Example: Using sqlite-vec, storing and querying is succinct:
  INSERT INTO embeddings(id, embedding) VALUES ('doc1_0', vec_f32('[0.1, 0.2, ...]'));
SELECT id, vec_distance_cosine(embedding, vec_f32('[0.1,0.2,...]')) 
  FROM embeddings 
  ORDER BY vec_distance_cosine(...) ASC LIMIT 5;
  The extension handles the heavy lifting in C with SIMD[31][36]. This is simpler than integrating an external library like FAISS, and it's highly portable (runs in the SQLite process)[30].
* MediaPipe Integration: Google's ML Kit offers ready models. For instance, to run text recognition on an image in Kotlin:
  val image = InputImage.fromBitmap(bitmap, rotation)
val recognizer = TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS)
recognizer.process(image)
  .addOnSuccessListener { visionText ->
       val detectedText = visionText.text
       // pass detectedText to LLM context
  }
  Similar approach for object detection (ML Kit has an ObjectDetector). These models use NNAPI so they are quite fast on modern devices.
* Tool Execution Example (Calendar):
  fun executeCreateEvent(title: String, datetime: String): Result {
    // parse datetime string to milliseconds
    val millis = parseDateTime(datetime) ?: return Result(error="Invalid date format")
    val intent = Intent(Intent.ACTION_INSERT).setData(CalendarContract.Events.CONTENT_URI)
    intent.putExtra(CalendarContract.Events.TITLE, title)
    intent.putExtra(CalendarContract.EXTRA_EVENT_BEGIN_TIME, millis)
    intent.putExtra(CalendarContract.EXTRA_EVENT_END_TIME, millis + 60*60*1000)
    intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK)
    return try {
        startActivity(intent)
        Result(ok=true)
    } catch(e: ActivityNotFoundException) {
        Result(error="No calendar app found")
    }
}
  Here Result is a simple data class we feed back to LLM if needed.
* Dynamic Shortcut Example:
  val shortcut = ShortcutInfo.Builder(context, "action_call_alice")
    .setShortLabel("Call Alice")
    .setLongLabel("Remind me to call Alice at 9 PM")
    .setIcon(Icon.createWithResource(context, R.drawable.ic_alarm))
    .setIntent(Intent(context, MainActivity::class.java).apply {
         action = "com.myapp.ACTION_REMIND_ALICE_9PM"
    }).build()
shortcutManager.pushDynamicShortcut(shortcut)
  If the user triggers that shortcut, our MainActivity checks the intent action and directly executes (or prompts) the corresponding tool.
* Safety Model Integration (Meta Guard): The 86M Llama Guard model is available on HuggingFace[49]. We will quantize it to 4-bit and load with llama.cpp (as it's basically a small transformer). We run it like:
  bool is_malicious = guardModel.evaluate(prompt) > 0.8; // pseudo: outputs score for malicious
if(is_malicious) {
    // refuse or modify prompt
}
  According to Detoxio's documentation, the model outputs a score for label_1 (malicious)[50]. We'll parse the logits accordingly.
* ADPF Thermal Headroom: Android's official example states:
  "Using ADPF, you can obtain CpuHeadroom which tells you how much more work you can do before performance suffers."[51]. In code, we'd do:
  val perfManager = context.getSystemService(PerformanceHintManager::class.java)
perfManager.reportActualWorkDuration(session, durationNanos) // for ongoing tasks
// We might not need to directly use this unless we tie into UI frame rendering, but for long computations, 
// we could break the work and yield if needed.
  More directly, ThermalManager:
  val thermalManager = context.getSystemService(ThermalService::class.java) as ThermalManager
thermalManager.addThermalStatusListener { status ->
    if(status >= ThermalManager.ThrottlingStatus.SEVERE) {
        // e.g., pause or lower activity
    }
}
Reference Projects: - The ExecuTorch Llama Android demo by Qualcomm shows building a PyTorch backend for Llama on Hexagon[14]. We glean from that that using Qualcomm's SDK is feasible, though we opt for ORT for simplicity. - Tarun Singh's article on Android Chat with llama.cpp and MLC[52] gave us insight into using OpenCL on Adreno vs. the TVM approach. We stick with llama.cpp as our engine due to its active support for mobile GPUs[53]. - Many open-source mobile AI apps (like "AlpacaChat" or "LocalAI Android") rely purely on CPU; our plan differentiates by using GPU and personal data RAG. We looked at a rising repo fork of Iris (called IRIS-Star)[54] which likely added some features; however, our design is largely custom-tailored to these new goals.
* Memory optimization: One can see vLLM's approach to KV in their GitHub - not directly applicable, but it inspired our thinking about non-contiguous cache[55].
Everything we plan has been informed by current research and best practices for on-device AI. We've combined knowledge from official docs, e.g., Qualcomm's OpenCL optimization note (which confirms Adreno's support and model quantization tip[7]) and SQLite-vec's documentation (which showcases how feasible local vector DB is[30]), to ensure our implementation is grounded in reality.
10. Implementation Roadmap (Phased Rollout)
To execute this ambitious project, we will break development into phases, each yielding a usable app increment and mitigating risks early:
Phase 1: MVP with Core Features (Focus: performance, RAG, voice) 1. Integrate llama.cpp with GPU - Duration: 2-3 weeks. - Setup the build system for Android, compile the library with OpenCL and Vulkan. - Implement JNI bindings to load a model and generate text. - Test on a Snapdragon (OpenCL) and Exynos (Vulkan) device with a small model (e.g. 3B) to confirm GPU acceleration works (verify tokens/s vs CPU). - Debug any issues (e.g. needing OpenCL ICD packaging). Risk: GPU backend might have driver quirks - we tackle this first because it's critical to performance. - Outcome: A basic console app or simple UI can load a model and run a prompt on GPU.
1. Model Manager & Download - Duration: 1 week.
2. Integrate model downloading (likely from HuggingFace via an in-app HTTP fetch, or allow sideloading). For now, maybe hardcode one model to speed things up.
3. UI: List of models (maybe just one in assets for MVP), ability to select and load it.
4. Ensure this respects storage (we might store models in app-specific storage or ask user to put on SD).
5. On-device Embeddings & RAG infra - Duration: 2 weeks.
6. Pick a small embedding model (maybe use the same LLM for embeddings temporarily to simplify, or a tiny model like all-MiniLM-L6 if converted).
7. Integrate sqlite-vec extension. Write Kotlin code to open a database and do a sample insert/query of vectors.
8. Create simple UI for user to input some reference text and index it. Then ask a question and fetch relevant text.
9. Initially, this can be command-line or very simple UI (for testing retrieval pipeline).
10. Risk: maybe sqlite-vec on Android (as an .so) - ensure we compile it for arm64 and it loads. We plan early so any issues with that extension are found now.
11. Outcome: Given a piece of text, the app can retrieve it when asked a related question (via semantic similarity).
12. Speech Integration - Duration: 1 week.
13. Add whisper.cpp JNI and test transcribing a short audio clip (we can embed a small audio file for test).
14. UI: mic button in the chat that starts recording, then stops and transcribes.
15. Also integrate system TTS for responses (just use Android's TextToSpeech).
16. This is relatively straightforward. The main risk is performance: check that Whisper tiny works realtime. If not, consider quantizing whisper model or using faster-whisper. But likely tiny.en will be okay.
17. Outcome: User can tap mic, speak a query, and see text output (no LLM yet in loop, just echoing transcript for now).
18. Chat UI & Basic QA - Duration: 2 weeks.
19. Build the chat conversation interface (message list, user vs assistant bubbles).
20. Connect the LLM generation to this UI: user enters text or uses voice, we generate response (with a loading indicator showing progress).
21. Implement streaming of tokens to the UI as they are generated (for better UX).
22. Incorporate the RAG retrieval: before sending the prompt to LLM, fetch top vectors and prepend to prompt. We might initially hardcode a simple prompt template for that.
23. Include basic stop/cancel ability (if generation is long, allow user to stop the thread).
24. This basically delivers a functional offline chat with knowledge augmentation and voice.
25. Test scenarios: simple Q&A, a question that uses indexed info ("What did I set as meeting notes yesterday?" after indexing that note), etc.
26. Address any glaring issues (e.g. formatting of retrieved text, or GPU memory OOM for long contexts - adjust context length if needed).
27. Prepare for a Phase 1 internal demo: We should have by now an app that showcases offline question answering with a small model. Performance tuning can continue, but core is there.
(At end of Phase 1, we should release an alpha build to some users to get feedback on basic functionality and performance, before adding complexity.)
Phase 2: Enhancing Intelligence and Safety (Focus: advanced generation, tools, moderation) 1. Speculative Decoding Prototype - Duration: 2 weeks (in parallel with other tasks). - Create a test harness for the draft model approach. Possibly use two smaller models for ease (like 7B as large, 3B as draft, on a PC or high-end phone). - Implement the logic to do one-step speculation (N=1 or 2). Measure speed gain. - If promising, integrate into the generation flow optionally. Possibly hide under a "Fast mode (experimental)" toggle. - This is high-risk: if it complicates code or yields minimal benefit, we might decide to defer or disable by default. But doing it in this phase ensures we know if it's viable.
1. Tool Invocation System - Duration: 3 weeks.
2. Define initial tool set (calendar, SMS, etc. as above).
3. Write the JSON schema and integrate into the prompt. Likely we fine-tune the system prompt to instruct model how to output JSON when needed (there might be examples from OpenAI function calling usage).
4. Implement JSON parsing in the app: after model generates, detect if output looks like tool JSON (or we might run a regex or a JSON parser on partial output).
5. Build the confirmation dialog flow in UI.
6. Implement each action's execution as needed (with proper permission requests).
7. Test with scenarios: e.g. user says "Remind me to buy milk at 6pm." - model hopefully picks create_calendar_event or set_alarm. We might need to nudge it with few-shot examples in the system prompt of how to use functions (since our model is not fine-tuned for this, it may require a little coaxing).
8. If results are inconsistent, consider a simpler approach: maybe avoid full JSON and instead use natural language extraction (less reliable). Ideally, we try to get the model to output a consistent format.
9. Risk: model might not follow format well since it wasn't trained on function calling. We may iterate with prompt engineering. If it fails, as fallback, we can parse intent from user using classic NLP (e.g. some rules: if sentence starts with "Remind me" -> calendar, etc.). But that defeats using LLM's reasoning. We aim to make it work with LLM to showcase its agent capability.
10. Small Models (Helper & Guard) Integration - Duration: 2 weeks.
11. Load and use the safety classifier model (Prompt Guard). Write a function bool checkPromptSafety(text) that returns false if disallowed. Place this before LLM generation.
12. Similarly, integrate a content filter for outputs. Perhaps reuse the guard model (it only covers jailbreak intent, not toxicity). We might get a tiny toxicity model or use a list of badwords as interim. Implement checkOutputSafety(text) to catch egregious outputs.
13. Provide toggles in settings to turn off these filters for advanced users (with warning).
14. Test: feed known problematic inputs (from OpenAI policy test cases) to ensure the model refuses or our filter catches them. E.g., "How to make a bomb" should be caught by prompt filter. "You're an idiot" from the assistant (if it ever did that) should be caught by output filter (or better, the model never does that because it's instructed).
15. Integrate the guard model execution via Hexagon if possible. This might align with next step:
16. Hexagon (QNN) Offload: Try to build ORT with QNN EP and run the guard model on it. If time doesn't permit, simply run guard on CPU for now. This offload can be optimized later.
17. Ensure all of this runs fast (the guard model should be quick, test its latency on device).
18. Provide a UI indicator if something was filtered ("Content removed" or so) for transparency.
19. Polish RAG & QA - Duration: 1 week.
20. Improve the prompt templates for RAG. Maybe format retrieved info as numbered bullet points and instruct the model to cite "(ref: 1)" or similar. While model might not perfectly do citations without fine-tune, this structure helps it use the info logically.
21. Implement deletion/updating of indexed data. E.g., if user says "clear my indexed data", drop tables and confirm. Or if an item is re-indexed, avoid duplication as described.
22. Add UI to view what's indexed (simple list from documents table perhaps) and allow deletion of specific entries.
23. Also ensure RAG can be toggled off (maybe user wants pure LLM sometimes).
24. Memory: if storing many embeddings, monitor memory usage of SQLite (should be fine, mostly disk storage).
25. UX Improvements - Duration: 1 week.
26. Add the dynamic shortcut creation for one or two obvious actions.
27. Ensure the conversation history is saved (so if app is closed and reopened, user sees past chat). Maybe limit history to recent 50 messages to avoid huge memory usage. (We'll eventually need to also not feed entire history to LLM every time if long - but with summarization or user-controlled reset).
28. Add a "New conversation" button to start fresh (clearing context).
29. Add a basic settings screen for: model selection, voice on/off, RAG on/off, safety level, etc.
30. Clean up UI: maybe some theming (dark mode), ensure it's accessible (talkback labels, etc.)
After Phase 2, we aim for a public beta release. At this point: - The app runs fully offline, uses GPU acceleration, supports voice, can perform a few system actions with confirmation, and has safety nets. - We should test it against the Play Store policies now: run a self-check using Google's checklists[41]. Fine-tune any responses that might borderline. Possibly involve some external testers to see if they can get it to do bad stuff, and adjust accordingly.
Phase 3: Advanced Features & Hardening (Focus: multimodal, optimization, compliance) 1. Image Input & Multimodal - Duration: 3 weeks. - Implement the image analysis pipeline. Start with a simple approach: allow user to attach an image in chat. When image is added, run MediaPipe detectors (object detection, OCR, maybe face description if needed). Then form a text description and feed to LLM as part of prompt. - If llama.cpp's libmtmd is stable and a small vision-enabled model is available, attempt loading one (maybe Gemma-3 1B, but note 1B with vision might not be very accurate; perhaps skip if not effective). - Test with different types of images (scenery, photo with text, etc.) to ensure it provides some useful context to LLM. - If device supports it, try using Pixel's on-device Vision APIs (like TextRecognizer we did, plus maybe Barcode scanning if relevant). - UI: show a thumbnail of image in chat and the assistant's description below it.
1. Thermal Balancing & Profiles - Duration: 2 weeks.
2. Incorporate reading of thermal status into the generation loop. If we get a "throttling" event mid-generation, decide how to handle: maybe gracefully shorten the response or slow down. This part is a bit experimental - we'll likely simulate by forcing high load and see if our listener triggers.
3. Provide a "Performance vs Battery" slider in settings: e.g. "Max Performance (runs model at full speed, may heat up)" vs "Balanced" vs "Battery Saver (slower, cooler)". Under the hood, this adjusts thread counts and model selection. For instance, Battery Saver mode might automatically use a 3B model even if 7B is available, and limit to 2 threads, etc. Balanced might use 7B but on fewer threads to reduce heat.
4. Test these modes on a real device by measuring token/s vs temperature (via ADB thermal info or using a FLIR camera if possible! Or internal temperature sensor readings).
5. Ensure foreground service usage is correct: for voice or continuous usage, we have the notification; when not needed, it's stopped (to avoid draining battery by keeping high priority).
6. App Compliance and Polishing - Duration: 1 week.
7. Review Google Play's latest guidelines. Particularly, ensure our privacy policy is accessible (we include a link in app about). Mention that all data is on device and not collected - this actually gives us a strong privacy stance, which is good.
8. Add disclaimers: possibly on first launch, a dialog "This AI may occasionally produce incorrect or offensive output. It does not represent any real person. Do not rely on it for medical, legal advice. By continuing, you agree to use at your own discretion." (This covers some legal bases and is often seen in AI apps).
9. Age gating: maybe put 17+ if we allow uncensored. But since our default is safe and no explicit sexual content allowed, we might go with a Teen rating.
10. Clean up any crashes or ANRs found in beta. Optimize memory (ensure large models are only loaded when needed, and unload on low memory).
11. Prepare store listing: screenshots, description highlighting offline and privacy, and describing some example uses (like voice Q&A, summarizing a note, setting a reminder).
12. Federated Learning hooks (future) - Not for initial release, but design in background.
13. We won't implement full federated learning in this timeline, but we want to keep the door open. This means: structure user persona memory in a way that could be used for fine-tuning small models later. Possibly log user feedback (if they rate responses) in a local file that could be used in a personal fine-tune.
14. Keep the core LLM model file unmodified (no on-device training yet, as that's too heavy). But maybe implement a simple LoRA apply mechanism for personalization: e.g. if user fine-tunes a LoRA on PC, they could drop it in and we apply it to model at runtime.
15. This is advanced and likely beyond initial release. We note it for future.
16. Final Testing - Duration: 1 week.
17. Test on multiple devices (especially a mid-range, a flagship, and a Pixel if possible due to differences).
18. Battery tests: run a long conversation and see battery drop over 15 minutes to extrapolate hourly drain. Ensure it's within reason (maybe 15-20% per hour heavy use on flagship - that might be acceptable given heavy compute).
19. Thermal tests: ensure device doesn't hit critical thermal shutdown in normal usage. If it does for certain models, adjust defaults.
20. Edge cases: no internet (app works fully offline anyway), device rotation (should probably lock orientation or handle UI state).
21. Verify all permissions flows (each tool action when first triggered asks permission and handles denial gracefully).
22. Security: ensure our intent handling can't be abused by other apps. For instance, our dynamic shortcuts or broadcast receivers - we should restrict any exported components so no external app can trigger our assistant to do something without user.
23. After this final pass, we'll be ready for the 1.0 release on Play Store.
High-Risk / Complex Items identified and how we handled: - GPU integration on Android - addressed early in Phase 1 to avoid finding out too late that it doesn't work on some devices. - Speculative decoding - very experimental, we time-boxed a prototype in Phase 2. If it fails, we can launch without it (just have it off). - Tool usage with LLM not fine-tuned for it - moderate risk that model just won't output correct JSON. If that happens, we may fall back to simpler intent detection, or restrict tool use to only very certain phrases. We would test with an open-source model known to follow formats (maybe try Llama-2-chat 7B, it has some ability to format answers). - Moderation accuracy - if our guard models aren't catching everything, it could be an issue. But since it's offline and user presumably consents to the experience, a rare slip might not be catastrophic. We'll fine-tune the thresholds and possibly include an update mechanism (we can update the safety model later without changing main model). - Performance on lower devices - We handle by providing smaller model options and battery mode.
By following this phased plan, we deliver incremental value: - Phase 1 gave a working offline chat with GPU boost and the beginnings of personal data integration - something we could already call unique. - Phase 2 layered in the "wow" features: performing actions on device and robust safety - making it actually useful as an assistant, not just QA. - Phase 3 polished the experience and ensured we meet all requirements and expand modalities.
We will continuously document and cite resources used in implementation (keeping track of license for any models and code). For instance, sqlite-vec is MIT licensed (fine to use), MediaPipe is Apache 2.0 (fine), llama.cpp is MIT. We'll include those attributions.
In conclusion, this design and plan provides a comprehensive path to transform iris_android from a basic local LLM app into a full-fledged on-device AI assistant: accelerated by hardware, enhanced by retrieval, multi-modal in capability, integrated with Android's ecosystem, and doing it all privately on the user's device. Each step of implementation keeps user experience, privacy, and compliance in mind, ensuring the final product is not only powerful but also trustworthy and user-friendly.

[1] [2] [30] [31] [32] [36] How sqlite-vec Works for Storing and Querying Vector Embeddings | by Stephen Collins | Medium
https://medium.com/@stephenc211/how-sqlite-vec-works-for-storing-and-querying-vector-embeddings-165adeeeceea
[3] [37] README.md
https://github.com/Telosnex/fllama/blob/5330251efc16b2631f13758f1462f729443b8d48/macos/llama.cpp/tools/mtmd/README.md
[4] [38] [39] [40] [43] [50] Llama Prompt Guards | Detoxio AI | Documentation
https://docs.detoxio.ai/redteam/guard-models/llama-prompt-guards/
[5] [6] [7] [8] [9] raw.githubusercontent.com
https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md
[10] [48] Introducing the New OpenCL GPU Backend in llama.cpp for ...
https://www.qualcomm.com/developer/blog/2024/11/introducing-new-opn-cl-gpu-backend-llama-cpp-for-qualcomm-adreno-gpu
[11] [15] [16] [17] Vulkan - GitHub
https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md
[12] GPU Performance Data Point via Vulkan · Issue #5410 - GitHub
https://github.com/ggerganov/llama.cpp/issues/5410
[13] GitHub - nerve-sparks/iris_android: IRIS is an android app for interfacing with GGUF / llama.cpp models locally.
https://github.com/nerve-sparks/iris_android
[14] Qualcomm AI Engine Backend - ExecuTorch 1.0 documentation
https://docs.pytorch.org/executorch/stable/backends-qualcomm.html
[18] [23] [25] [26] Speculation
https://huggingface.co/docs/text-generation-inference/en/conceptual/speculation
[19] [20] [21] [22] OpenAI new feature 'Predicted Outputs' uses speculative decoding : r/LocalLLaMA
https://www.reddit.com/r/LocalLLaMA/comments/1gjzmjp/openai_new_feature_predicted_outputs_uses/
[24] Recurrent Drafter for Fast Speculative Decoding in Large Language ...
https://arxiv.org/abs/2403.09919
[27] [28] [29] [55] PagedAttention
https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention
[33] [34] [35] On-device vector database for Android (Java, Kotlin)
https://objectbox.io/the-on-device-vector-database-for-android-and-java/
[41] [42] Understanding Google Play's AI-Generated Content policy - Play Console Help
https://support.google.com/googleplay/android-developer/answer/14094294?hl=en
[44] Getting started with Android Dynamic Performance Framework ...
https://developer.android.com/stories/games/arm-adpf
[45] [51] The Second Beta of Android 16 - Android Developers Blog
https://android-developers.googleblog.com/2025/02/second-beta-android16.html
[46] [PDF] Introduction to the Qualcomm(r) AdrenoTM Optimized ... - iwocl
https://www.iwocl.org/wp-content/uploads/iwocl-2025-hongqiang-wang-lamacpp-backend-update.pdf
[47] Best Practices to Safeguard AI-Generated Content - Google Help
https://support.google.com/googleplay/android-developer/answer/16353813?hl=en
[49] meta-llama/Llama-Prompt-Guard-2-86M - Hugging Face
https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M
[52] [53] On-Device AI Chat & Translate on Android (Qualcomm GENIE, MLC, WebLLM): Your Phone, Your LLM | by Tarun Singh | Sep, 2025 | Towards AI
https://pub.towardsai.net/on-device-ai-chat-translate-on-android-qualcomm-genie-mlc-webllm-your-phone-your-llm-49594aff3b9f?gi=7738eed11bbe
[54] on-device-ai | Topic | Ecosyste.ms: Repos
https://repos.ecosyste.ms/topics/on-device-ai
