# Issue #7.5: Multimodal Production Implementation

## üéØ Epic: Production-Ready Multimodal Engine
**Priority**: P1 (High)  
**Estimate**: 8-12 days  
**Dependencies**: Issue #07 (Foundation Complete), #02 (Native llama.cpp), #04 (Model Management)  
**Architecture Reference**: [docs/architecture.md](../architecture.md) - Section 7 Multimodal Engine  
**Foundation**: [docs/multimodal-integration-guide.md](../multimodal-integration-guide.md)

## üìã Overview
Implement production-ready multimodal AI capabilities by replacing mock implementations with real vision processing, model registry, and image handling. This builds on the completed foundation from Issue #07 to deliver actual vision-language model support for LLaVA, Qwen-VL, and other multimodal models.

## üéØ Goals
- **Real Model Registry**: Device-aware model recommendations with actual compatibility assessment
- **Production Image Processing**: Real image preprocessing with Android native libraries  
- **Native Vision Engine**: Integration with llama.cpp for actual vision-language inference
- **Performance Optimization**: Memory management, caching, and device-specific tuning
- **Comprehensive Testing**: End-to-end validation with real models and images

## üìã Prerequisites & Foundation Status

### ‚úÖ Completed Foundation (Issue #07)
- Module structure: `core-multimodal/` with proper Gradle configuration
- Complete interface definitions in `MultimodalInterfaces.kt`
- Full type system in `MultimodalTypes.kt`  
- Working mock implementations: `MockMultimodalModelRegistry`, `MockImageProcessor`, `MockVisionProcessingEngine`
- Build verification: Module compiles successfully
- Integration documentation: Complete API guide and usage examples

### üîÑ Required Dependencies
- **Issue #02**: Native llama.cpp integration must be complete for vision model loading
- **Issue #04**: Model management system for device profiling and model catalog
- **Core LLM Module**: `core-llm` module with JNI bridge functional

## üìù Detailed Implementation Tasks

### 1. Production Model Registry

#### 1.1 Replace MockMultimodalModelRegistry
**File**: `core-multimodal/src/main/kotlin/com/nervesparks/iris/core/multimodal/registry/MultimodalModelRegistryImpl.kt`

```kotlin
@Singleton
class MultimodalModelRegistryImpl @Inject constructor(
    private val deviceProfileProvider: DeviceProfileProvider,
    private val modelCatalogService: ModelCatalogService,
    @ApplicationContext private val context: Context,
    @IoDispatcher private val ioDispatcher: CoroutineDispatcher
) : MultimodalModelRegistry {
    
    companion object {
        private const val TAG = "MultimodalModelRegistry"
        private const val MULTIMODAL_CATALOG_FILE = "multimodal_models.json"
        
        // Model compatibility scoring weights
        private const val MEMORY_WEIGHT = 0.4f
        private const val PERFORMANCE_WEIGHT = 0.3f
        private const val FEATURE_WEIGHT = 0.2f
        private const val DEVICE_CLASS_WEIGHT = 0.1f
    }
    
    private val modelCache = ConcurrentHashMap<String, MultimodalModelDescriptor>()
    private val compatibilityCache = ConcurrentHashMap<String, MultimodalModelCompatibilityAssessment>()
    
    override suspend fun getRecommendedModel(visionTask: VisionTask): Result<MultimodalModelDescriptor> = 
        withContext(ioDispatcher) {
            try {
                Log.d(TAG, "Getting recommended model for task: $visionTask")
                
                val availableModels = getAvailableModels().getOrThrow()
                val deviceProfile = deviceProfileProvider.getDeviceProfile()
                
                // Filter models that support the requested task
                val supportingModels = availableModels.filter { model ->
                    when (visionTask) {
                        VisionTask.OBJECT_DETECTION -> model.capabilities.contains(MultimodalCapability.OBJECT_DETECTION)
                        VisionTask.TEXT_RECOGNITION -> model.capabilities.contains(MultimodalCapability.TEXT_RECOGNITION)
                        VisionTask.IMAGE_CLASSIFICATION -> model.capabilities.contains(MultimodalCapability.IMAGE_CLASSIFICATION)
                        VisionTask.SCENE_ANALYSIS -> model.capabilities.contains(MultimodalCapability.SCENE_ANALYSIS)
                        VisionTask.GENERAL_QA -> model.capabilities.contains(MultimodalCapability.VISUAL_QUESTION_ANSWERING)
                    }
                }
                
                if (supportingModels.isEmpty()) {
                    return@withContext Result.failure(
                        IllegalArgumentException("No models available for task: $visionTask")
                    )
                }
                
                // Score and rank models
                val scoredModels = supportingModels.map { model ->
                    val compatibility = assessModelCompatibility(model).getOrThrow()
                    Pair(model, compatibility.compatibilityScore)
                }.sortedByDescending { it.second }
                
                val bestModel = scoredModels.firstOrNull()?.first
                    ?: return@withContext Result.failure(
                        IllegalStateException("No compatible model found")
                    )
                
                Log.i(TAG, "Recommended model: ${bestModel.id} for task: $visionTask")
                Result.success(bestModel)
                
            } catch (e: Exception) {
                Log.e(TAG, "Failed to get recommended model", e)
                Result.failure(MultimodalInferenceException("Model recommendation failed", e))
            }
        }
    
    override suspend fun assessModelCompatibility(model: MultimodalModelDescriptor): Result<MultimodalModelCompatibilityAssessment> =
        withContext(ioDispatcher) {
            try {
                // Check cache first
                compatibilityCache[model.id]?.let { cached ->
                    return@withContext Result.success(cached)
                }
                
                val deviceProfile = deviceProfileProvider.getDeviceProfile()
                val assessment = calculateCompatibilityScore(model, deviceProfile)
                
                // Cache the result
                compatibilityCache[model.id] = assessment
                
                Result.success(assessment)
                
            } catch (e: Exception) {
                Log.e(TAG, "Compatibility assessment failed for model: ${model.id}", e)
                Result.failure(MultimodalInferenceException("Compatibility assessment failed", e))
            }
        }
    
    override suspend fun getAvailableModels(): Result<List<MultimodalModelDescriptor>> =
        withContext(ioDispatcher) {
            try {
                // Load from catalog if cache is empty
                if (modelCache.isEmpty()) {
                    loadModelCatalog()
                }
                
                Result.success(modelCache.values.toList())
                
            } catch (e: Exception) {
                Log.e(TAG, "Failed to load available models", e)
                Result.failure(MultimodalInferenceException("Failed to load models", e))
            }
        }
    
    override suspend fun getModelById(modelId: String): Result<MultimodalModelDescriptor> =
        withContext(ioDispatcher) {
            try {
                modelCache[modelId]?.let { model ->
                    return@withContext Result.success(model)
                }
                
                // Try to load from catalog
                loadModelCatalog()
                
                modelCache[modelId]?.let { model ->
                    Result.success(model)
                } ?: Result.failure(
                    IllegalArgumentException("Model not found: $modelId")
                )
                
            } catch (e: Exception) {
                Log.e(TAG, "Failed to get model by ID: $modelId", e)
                Result.failure(MultimodalInferenceException("Model lookup failed", e))
            }
        }
    
    private suspend fun loadModelCatalog() {
        try {
            val catalogStream = context.assets.open(MULTIMODAL_CATALOG_FILE)
            val catalogJson = catalogStream.bufferedReader().use { it.readText() }
            
            // Parse the catalog (assuming JSON format)
            val catalog = Json.decodeFromString<MultimodalModelCatalog>(catalogJson)
            
            // Populate cache
            catalog.models.forEach { model ->
                modelCache[model.id] = model
            }
            
            Log.i(TAG, "Loaded ${catalog.models.size} models from catalog")
            
        } catch (e: Exception) {
            Log.e(TAG, "Failed to load model catalog", e)
            // Load default fallback models
            loadFallbackModels()
        }
    }
    
    private fun loadFallbackModels() {
        val fallbackModel = MultimodalModelDescriptor(
            id = "llava-1.5-7b-q4",
            name = "LLaVA 1.5 7B (Q4)",
            baseModel = "vicuna-7b-v1.5",
            visionRequirements = VisionRequirements(
                maxImageSize = ImageSize(512, 512),
                supportedFormats = listOf(ImageFormat.JPEG, ImageFormat.PNG),
                minConfidence = 0.6f
            ),
            supportedImageFormats = listOf(ImageFormat.JPEG, ImageFormat.PNG, ImageFormat.WEBP),
            performance = ModelPerformance(
                inferenceTimeMs = 800L,
                memoryUsageMB = 4096,
                accuracy = 0.82f
            ),
            capabilities = listOf(
                MultimodalCapability.VISUAL_QUESTION_ANSWERING,
                MultimodalCapability.IMAGE_CLASSIFICATION,
                MultimodalCapability.SCENE_ANALYSIS
            )
        )
        
        modelCache[fallbackModel.id] = fallbackModel
        Log.w(TAG, "Loaded fallback model: ${fallbackModel.id}")
    }
    
    private fun calculateCompatibilityScore(
        model: MultimodalModelDescriptor,
        deviceProfile: DeviceProfile
    ): MultimodalModelCompatibilityAssessment {
        var score = 0.0
        val issues = mutableListOf<String>()
        
        // Memory compatibility check
        val requiredMemory = model.performance.memoryUsageMB * 1024L * 1024L
        val availableMemory = deviceProfile.availableRAM
        
        when {
            availableMemory >= requiredMemory * 1.5 -> score += MEMORY_WEIGHT * 1.0
            availableMemory >= requiredMemory * 1.2 -> score += MEMORY_WEIGHT * 0.8
            availableMemory >= requiredMemory -> score += MEMORY_WEIGHT * 0.6
            else -> {
                score += MEMORY_WEIGHT * 0.2
                issues.add("Insufficient memory: need ${requiredMemory / (1024*1024)}MB, have ${availableMemory / (1024*1024)}MB")
            }
        }
        
        // Performance compatibility
        val expectedInferenceTime = model.performance.inferenceTimeMs
        when {
            expectedInferenceTime <= 500 -> score += PERFORMANCE_WEIGHT * 1.0
            expectedInferenceTime <= 1000 -> score += PERFORMANCE_WEIGHT * 0.8
            expectedInferenceTime <= 2000 -> score += PERFORMANCE_WEIGHT * 0.6
            else -> {
                score += PERFORMANCE_WEIGHT * 0.4
                issues.add("Slow inference expected: ${expectedInferenceTime}ms")
            }
        }
        
        // Feature compatibility
        val supportedCapabilities = model.capabilities.size
        when {
            supportedCapabilities >= 5 -> score += FEATURE_WEIGHT * 1.0
            supportedCapabilities >= 3 -> score += FEATURE_WEIGHT * 0.8
            supportedCapabilities >= 2 -> score += FEATURE_WEIGHT * 0.6
            else -> score += FEATURE_WEIGHT * 0.4
        }
        
        // Device class bonus
        when (deviceProfile.deviceClass) {
            DeviceClass.FLAGSHIP -> score += DEVICE_CLASS_WEIGHT * 1.0
            DeviceClass.HIGH_END -> score += DEVICE_CLASS_WEIGHT * 0.8
            DeviceClass.MID_RANGE -> score += DEVICE_CLASS_WEIGHT * 0.6
            DeviceClass.BUDGET -> score += DEVICE_CLASS_WEIGHT * 0.4
        }
        
        val isSupported = score >= 0.5 && availableMemory >= requiredMemory
        
        return MultimodalModelCompatibilityAssessment(
            isSupported = isSupported,
            compatibilityScore = score,
            memoryRequirement = requiredMemory,
            reasonsForIncompatibility = if (isSupported) emptyList() else issues
        )
    }
}

@Serializable
data class MultimodalModelCatalog(
    val version: String,
    val lastUpdated: Long,
    val models: List<MultimodalModelDescriptor>
)
```

#### 1.2 Model Catalog Asset
**File**: `core-multimodal/src/main/assets/multimodal_models.json`

```json
{
  "version": "1.0.0",
  "lastUpdated": 1699747200000,
  "models": [
    {
      "id": "llava-1.5-7b-q4",
      "name": "LLaVA 1.5 7B (Q4)",
      "baseModel": "vicuna-7b-v1.5",
      "visionRequirements": {
        "maxImageSize": {"width": 512, "height": 512},
        "supportedFormats": ["JPEG", "PNG"],
        "minConfidence": 0.6
      },
      "supportedImageFormats": ["JPEG", "PNG", "WEBP"],
      "performance": {
        "inferenceTimeMs": 800,
        "memoryUsageMB": 4096,
        "accuracy": 0.82
      },
      "capabilities": [
        "VISUAL_QUESTION_ANSWERING",
        "IMAGE_CLASSIFICATION", 
        "SCENE_ANALYSIS"
      ]
    },
    {
      "id": "qwen-vl-chat-q4",
      "name": "Qwen-VL-Chat (Q4)",
      "baseModel": "qwen-7b-chat",
      "visionRequirements": {
        "maxImageSize": {"width": 448, "height": 448},
        "supportedFormats": ["JPEG", "PNG"],
        "minConfidence": 0.65
      },
      "supportedImageFormats": ["JPEG", "PNG"],
      "performance": {
        "inferenceTimeMs": 750,
        "memoryUsageMB": 3800,
        "accuracy": 0.85
      },
      "capabilities": [
        "VISUAL_QUESTION_ANSWERING",
        "TEXT_RECOGNITION",
        "DOCUMENT_ANALYSIS"
      ]
    }
  ]
}
```

### 2. Production Image Processing

#### 2.1 Replace MockImageProcessor  
**File**: `core-multimodal/src/main/kotlin/com/nervesparks/iris/core/multimodal/image/ImageProcessorImpl.kt`

```kotlin
@Singleton
class ImageProcessorImpl @Inject constructor(
    @ApplicationContext private val context: Context,
    @IoDispatcher private val ioDispatcher: CoroutineDispatcher
) : ImageProcessor {
    
    companion object {
        private const val TAG = "ImageProcessor"
        private const val MAX_IMAGE_SIZE_BYTES = 10 * 1024 * 1024 // 10MB
        private const val JPEG_QUALITY = 85
        
        private val SUPPORTED_MIME_TYPES = setOf(
            "image/jpeg", "image/png", "image/webp", "image/bmp"
        )
    }
    
    override suspend fun preprocessImage(
        uri: Uri, 
        targetSize: Int, 
        format: ImageFormat
    ): Result<ProcessedImageData> = withContext(ioDispatcher) {
        try {
            Log.d(TAG, "Preprocessing image: $uri, targetSize: $targetSize, format: $format")
            
            // Load the original image
            val originalBitmap = loadBitmapFromUri(uri)
                ?: return@withContext Result.failure(
                    IllegalArgumentException("Failed to load image from URI: $uri")
                )
            
            // Resize to target dimensions while maintaining aspect ratio
            val resizedBitmap = resizeBitmapToTarget(originalBitmap, targetSize)
            
            // Convert to the requested format and get byte array
            val imageBytes = compressBitmapToBytes(resizedBitmap, format)
            
            val processedData = ProcessedImageData(
                data = imageBytes,
                format = format,
                width = resizedBitmap.width,
                height = resizedBitmap.height,
                channels = when (format) {
                    ImageFormat.PNG -> if (hasAlpha(resizedBitmap)) 4 else 3
                    else -> 3
                }
            )
            
            // Cleanup
            originalBitmap.recycle()
            resizedBitmap.recycle()
            
            Log.i(TAG, "Image preprocessed successfully: ${processedData.width}x${processedData.height}, ${imageBytes.size} bytes")
            Result.success(processedData)
            
        } catch (e: Exception) {
            Log.e(TAG, "Image preprocessing failed", e)
            Result.failure(MultimodalInferenceException("Image preprocessing failed", e))
        }
    }
    
    override suspend fun validateImage(uri: Uri): Result<Boolean> = withContext(ioDispatcher) {
        try {
            val contentResolver = context.contentResolver
            
            // Check if URI is accessible
            val inputStream = contentResolver.openInputStream(uri)
                ?: return@withContext Result.success(false)
            
            inputStream.use { stream ->
                // Check file size
                val fileSize = stream.available()
                if (fileSize > MAX_IMAGE_SIZE_BYTES) {
                    Log.w(TAG, "Image too large: $fileSize bytes")
                    return@withContext Result.success(false)
                }
                
                // Check MIME type
                val mimeType = contentResolver.getType(uri)
                if (mimeType == null || !SUPPORTED_MIME_TYPES.contains(mimeType)) {
                    Log.w(TAG, "Unsupported MIME type: $mimeType")
                    return@withContext Result.success(false)
                }
                
                // Try to decode the image to verify it's valid
                val options = BitmapFactory.Options().apply {
                    inJustDecodeBounds = true
                }
                
                stream.mark(stream.available())
                BitmapFactory.decodeStream(stream, null, options)
                stream.reset()
                
                val isValid = options.outWidth > 0 && options.outHeight > 0
                
                Log.d(TAG, "Image validation result: $isValid for $uri")
                Result.success(isValid)
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "Image validation failed", e)
            Result.success(false)
        }
    }
    
    private fun loadBitmapFromUri(uri: Uri): Bitmap? {
        return try {
            val inputStream = context.contentResolver.openInputStream(uri)
            inputStream?.use { stream ->
                BitmapFactory.decodeStream(stream)
            }
        } catch (e: Exception) {
            Log.e(TAG, "Failed to load bitmap from URI", e)
            null
        }
    }
    
    private fun resizeBitmapToTarget(bitmap: Bitmap, targetSize: Int): Bitmap {
        val width = bitmap.width
        val height = bitmap.height
        
        // Calculate scale factor to maintain aspect ratio
        val scaleFactor = minOf(
            targetSize.toFloat() / width,
            targetSize.toFloat() / height
        )
        
        val newWidth = (width * scaleFactor).toInt()
        val newHeight = (height * scaleFactor).toInt()
        
        return Bitmap.createScaledBitmap(bitmap, newWidth, newHeight, true)
    }
    
    private fun compressBitmapToBytes(bitmap: Bitmap, format: ImageFormat): ByteArray {
        val outputStream = ByteArrayOutputStream()
        
        val compressFormat = when (format) {
            ImageFormat.JPEG -> Bitmap.CompressFormat.JPEG
            ImageFormat.PNG -> Bitmap.CompressFormat.PNG
            ImageFormat.WEBP -> Bitmap.CompressFormat.WEBP
            ImageFormat.BMP -> Bitmap.CompressFormat.PNG // BMP not directly supported, use PNG
        }
        
        val quality = when (format) {
            ImageFormat.JPEG -> JPEG_QUALITY
            ImageFormat.WEBP -> JPEG_QUALITY
            else -> 100
        }
        
        bitmap.compress(compressFormat, quality, outputStream)
        return outputStream.toByteArray()
    }
    
    private fun hasAlpha(bitmap: Bitmap): Boolean {
        return bitmap.config == Bitmap.Config.ARGB_8888 && bitmap.hasAlpha()
    }
}
```

### 3. Production Vision Processing Engine

#### 3.1 Replace MockVisionProcessingEngine
**File**: `core-multimodal/src/main/kotlin/com/nervesparks/iris/core/multimodal/vision/VisionProcessingEngineImpl.kt`

```kotlin
@Singleton
class VisionProcessingEngineImpl @Inject constructor(
    private val nativeInferenceEngine: NativeInferenceEngine,
    private val imageProcessor: ImageProcessor,
    private val performanceMonitor: PerformanceMonitor,
    @ApplicationContext private val context: Context,
    @IoDispatcher private val ioDispatcher: CoroutineDispatcher
) : VisionProcessingEngine {
    
    companion object {
        private const val TAG = "VisionProcessingEngine"
        private const val VISION_MODEL_CACHE_SIZE = 2
        private const val DEFAULT_TIMEOUT_MS = 30_000L
    }
    
    private val loadedModels = ConcurrentHashMap<String, LoadedVisionModel>()
    private val visionModelCache = LruCache<String, LoadedVisionModel>(VISION_MODEL_CACHE_SIZE)
    
    override suspend fun analyzeImage(
        imageUri: Uri,
        prompt: String,
        model: MultimodalModelDescriptor,
        parameters: VisionParameters
    ): Result<VisionResult.AnalysisResult> = withContext(ioDispatcher) {
        try {
            Log.d(TAG, "Starting image analysis with model: ${model.id}")
            val startTime = System.currentTimeMillis()
            
            // Validate and preprocess image
            val imageValidation = imageProcessor.validateImage(imageUri)
            if (!imageValidation.getOrElse { false }) {
                return@withContext Result.failure(
                    IllegalArgumentException("Invalid image: $imageUri")
                )
            }
            
            val processedImage = imageProcessor.preprocessImage(
                uri = imageUri,
                targetSize = model.visionRequirements.maxImageSize.width,
                format = model.supportedImageFormats.first()
            ).getOrThrow()
            
            // Load vision model if needed
            val visionModel = loadVisionModel(model).getOrThrow()
            
            // Build vision prompt
            val visionPrompt = VisionPrompt(
                text = prompt,
                imageData = processedImage,
                systemPrompt = buildSystemPrompt(VisionTask.GENERAL_QA)
            )
            
            // Perform inference
            val inferenceResult = performVisionInference(visionModel, visionPrompt, parameters)
            
            val processingTime = System.currentTimeMillis() - startTime
            
            val result = VisionResult.AnalysisResult(
                text = inferenceResult.text,
                confidence = inferenceResult.confidence,
                processingTimeMs = processingTime
            )
            
            Log.i(TAG, "Image analysis completed in ${processingTime}ms with confidence ${inferenceResult.confidence}")
            Result.success(result)
            
        } catch (e: Exception) {
            Log.e(TAG, "Image analysis failed", e)
            Result.failure(MultimodalInferenceException("Image analysis failed", e))
        }
    }
    
    override suspend fun processScreenshot(
        screenshotData: ByteArray,
        prompt: String,
        model: MultimodalModelDescriptor,
        parameters: VisionParameters
    ): Result<VisionResult.ScreenshotResult> = withContext(ioDispatcher) {
        try {
            Log.d(TAG, "Processing screenshot with model: ${model.id}")
            val startTime = System.currentTimeMillis()
            
            // Convert screenshot data to bitmap and process
            val bitmap = BitmapFactory.decodeByteArray(screenshotData, 0, screenshotData.size)
                ?: return@withContext Result.failure(
                    IllegalArgumentException("Failed to decode screenshot data")
                )
            
            // Convert to processed image data
            val outputStream = ByteArrayOutputStream()
            bitmap.compress(Bitmap.CompressFormat.JPEG, 85, outputStream)
            
            val processedImage = ProcessedImageData(
                data = outputStream.toByteArray(),
                format = ImageFormat.JPEG,
                width = bitmap.width,
                height = bitmap.height,
                channels = 3
            )
            
            // Load vision model
            val visionModel = loadVisionModel(model).getOrThrow()
            
            // Build screenshot-specific prompt
            val screenPrompt = buildScreenshotPrompt(prompt)
            val visionPrompt = VisionPrompt(
                text = screenPrompt,
                imageData = processedImage,
                systemPrompt = buildSystemPrompt(VisionTask.SCENE_ANALYSIS)
            )
            
            // Perform inference
            val inferenceResult = performVisionInference(visionModel, visionPrompt, parameters)
            
            val processingTime = System.currentTimeMillis() - startTime
            
            // Parse UI elements and text regions from response
            val (uiElements, textRegions) = parseScreenshotAnalysis(inferenceResult.text)
            
            val result = VisionResult.ScreenshotResult(
                text = inferenceResult.text,
                confidence = inferenceResult.confidence,
                processingTimeMs = processingTime,
                uiElements = uiElements,
                textRegions = textRegions
            )
            
            Log.i(TAG, "Screenshot processing completed in ${processingTime}ms")
            Result.success(result)
            
        } catch (e: Exception) {
            Log.e(TAG, "Screenshot processing failed", e)
            Result.failure(MultimodalInferenceException("Screenshot processing failed", e))
        }
    }
    
    override suspend fun extractTextFromImage(
        imageUri: Uri,
        model: MultimodalModelDescriptor,
        parameters: VisionParameters
    ): Result<VisionResult.OCRResult> = withContext(ioDispatcher) {
        try {
            // Check if model supports OCR
            if (!model.capabilities.contains(MultimodalCapability.TEXT_RECOGNITION)) {
                return@withContext Result.failure(
                    IllegalArgumentException("Model does not support OCR: ${model.id}")
                )
            }
            
            Log.d(TAG, "Extracting text from image with model: ${model.id}")
            val startTime = System.currentTimeMillis()
            
            // Validate and preprocess image
            val processedImage = imageProcessor.preprocessImage(
                uri = imageUri,
                targetSize = model.visionRequirements.maxImageSize.width,
                format = ImageFormat.JPEG
            ).getOrThrow()
            
            // Load vision model
            val visionModel = loadVisionModel(model).getOrThrow()
            
            // Build OCR prompt
            val ocrPrompt = VisionPrompt(
                text = "Extract all visible text from this image. Provide the text exactly as it appears.",
                imageData = processedImage,
                systemPrompt = buildSystemPrompt(VisionTask.TEXT_RECOGNITION)
            )
            
            // Perform inference
            val inferenceResult = performVisionInference(visionModel, ocrPrompt, parameters)
            
            val processingTime = System.currentTimeMillis() - startTime
            
            // Parse text regions
            val textRegions = parseTextRegions(inferenceResult.text)
            
            val result = VisionResult.OCRResult(
                extractedText = inferenceResult.text,
                confidence = inferenceResult.confidence,
                processingTimeMs = processingTime,
                textRegions = textRegions
            )
            
            Log.i(TAG, "OCR completed in ${processingTime}ms")
            Result.success(result)
            
        } catch (e: Exception) {
            Log.e(TAG, "OCR failed", e)
            Result.failure(MultimodalInferenceException("OCR failed", e))
        }
    }
    
    override suspend fun analyzeDocument(
        imageUri: Uri,
        documentType: DocumentType,
        prompt: String,
        model: MultimodalModelDescriptor,
        parameters: VisionParameters
    ): Result<VisionResult.DocumentResult> = withContext(ioDispatcher) {
        try {
            Log.d(TAG, "Analyzing document of type: $documentType")
            val startTime = System.currentTimeMillis()
            
            // Preprocess image with document-optimized settings
            val processedImage = imageProcessor.preprocessImage(
                uri = imageUri,
                targetSize = model.visionRequirements.maxImageSize.width,
                format = ImageFormat.JPEG
            ).getOrThrow()
            
            // Load vision model
            val visionModel = loadVisionModel(model).getOrThrow()
            
            // Build document-specific prompt
            val documentPrompt = buildDocumentPrompt(documentType, prompt)
            val visionPrompt = VisionPrompt(
                text = documentPrompt,
                imageData = processedImage,
                systemPrompt = buildSystemPrompt(VisionTask.SCENE_ANALYSIS)
            )
            
            // Perform inference
            val inferenceResult = performVisionInference(visionModel, visionPrompt, parameters)
            
            val processingTime = System.currentTimeMillis() - startTime
            
            // Parse structured data based on document type
            val structuredData = parseDocumentData(inferenceResult.text, documentType)
            
            val result = VisionResult.DocumentResult(
                extractedText = inferenceResult.text,
                confidence = inferenceResult.confidence,
                processingTimeMs = processingTime,
                documentType = documentType,
                structuredData = structuredData
            )
            
            Log.i(TAG, "Document analysis completed in ${processingTime}ms")
            Result.success(result)
            
        } catch (e: Exception) {
            Log.e(TAG, "Document analysis failed", e)
            Result.failure(MultimodalInferenceException("Document analysis failed", e))
        }
    }
    
    override fun streamVisionAnalysis(
        imageUri: Uri,
        prompt: String,
        model: MultimodalModelDescriptor,
        parameters: VisionParameters
    ): Flow<VisionResult.StreamResult> = flow {
        try {
            emit(VisionResult.StreamResult.Started)
            
            // Preprocess image
            val processedImage = imageProcessor.preprocessImage(
                uri = imageUri,
                targetSize = model.visionRequirements.maxImageSize.width,
                format = model.supportedImageFormats.first()
            ).getOrThrow()
            
            // Load vision model
            val visionModel = loadVisionModel(model).getOrThrow()
            
            // Build vision prompt
            val visionPrompt = VisionPrompt(
                text = prompt,
                imageData = processedImage,
                systemPrompt = buildSystemPrompt(VisionTask.GENERAL_QA)
            )
            
            // Stream inference results
            nativeInferenceEngine.streamVisionInference(visionModel.nativeHandle, visionPrompt)
                .collect { streamResult ->
                    when (streamResult) {
                        is NativeVisionStreamResult.TextChunk -> {
                            emit(VisionResult.StreamResult.TextChunk(
                                text = streamResult.text,
                                isComplete = false
                            ))
                        }
                        is NativeVisionStreamResult.Completed -> {
                            val finalResult = VisionResult.AnalysisResult(
                                text = streamResult.fullText,
                                confidence = streamResult.confidence,
                                processingTimeMs = streamResult.processingTimeMs
                            )
                            emit(VisionResult.StreamResult.Completed(finalResult))
                        }
                        is NativeVisionStreamResult.Error -> {
                            emit(VisionResult.StreamResult.Error(
                                MultimodalInferenceException(streamResult.message)
                            ))
                        }
                    }
                }
                
        } catch (e: Exception) {
            Log.e(TAG, "Streaming vision analysis failed", e)
            emit(VisionResult.StreamResult.Error(
                MultimodalInferenceException("Streaming analysis failed", e)
            ))
        }
    }.flowOn(ioDispatcher)
    
    private suspend fun loadVisionModel(model: MultimodalModelDescriptor): Result<LoadedVisionModel> {
        // Check cache first
        loadedModels[model.id]?.let { cached ->
            return Result.success(cached)
        }
        
        return try {
            Log.i(TAG, "Loading vision model: ${model.id}")
            
            val modelPath = getModelPath(model.baseModel)
            val nativeHandle = nativeInferenceEngine.loadVisionModel(
                modelPath = modelPath,
                visionConfig = VisionConfig(
                    maxImageSize = model.visionRequirements.maxImageSize,
                    supportedFormats = model.supportedImageFormats
                )
            ).getOrThrow()
            
            val loadedModel = LoadedVisionModel(
                descriptor = model,
                nativeHandle = nativeHandle,
                loadTime = System.currentTimeMillis()
            )
            
            // Cache the loaded model
            loadedModels[model.id] = loadedModel
            visionModelCache.put(model.id, loadedModel)
            
            Log.i(TAG, "Vision model loaded successfully: ${model.id}")
            Result.success(loadedModel)
            
        } catch (e: Exception) {
            Log.e(TAG, "Failed to load vision model: ${model.id}", e)
            Result.failure(MultimodalInferenceException("Model loading failed", e))
        }
    }
    
    private suspend fun performVisionInference(
        visionModel: LoadedVisionModel,
        prompt: VisionPrompt,
        parameters: VisionParameters
    ): VisionInferenceResult {
        
        performanceMonitor.startVisionInference(visionModel.descriptor.id)
        
        try {
            val result = nativeInferenceEngine.performVisionInference(
                modelHandle = visionModel.nativeHandle,
                prompt = prompt,
                parameters = parameters
            ).getOrThrow()
            
            performanceMonitor.endVisionInference(visionModel.descriptor.id, true)
            return result
            
        } catch (e: Exception) {
            performanceMonitor.endVisionInference(visionModel.descriptor.id, false)
            throw e
        }
    }
    
    private fun buildSystemPrompt(task: VisionTask): String {
        return when (task) {
            VisionTask.OBJECT_DETECTION -> "You are an expert at detecting and identifying objects in images. Provide detailed descriptions of all visible objects and their locations."
            VisionTask.TEXT_RECOGNITION -> "You are an OCR expert. Extract all visible text exactly as it appears in the image, maintaining formatting and structure."
            VisionTask.IMAGE_CLASSIFICATION -> "You are an image classification expert. Categorize the main subject and provide detailed analysis of the image content."
            VisionTask.SCENE_ANALYSIS -> "You are a scene analysis expert. Describe the overall scene, setting, activities, and context visible in the image."
            VisionTask.GENERAL_QA -> "You are a helpful assistant that can understand and analyze images. Answer questions about the image content accurately and in detail."
        }
    }
    
    private fun buildScreenshotPrompt(userPrompt: String): String {
        return """
            This is a screenshot from a mobile application or webpage. 
            Please analyze the user interface elements, text content, and overall layout.
            
            User's specific question: $userPrompt
            
            Provide information about:
            1. Main UI elements visible (buttons, text fields, images, etc.)
            2. Text content that can be read
            3. Overall purpose or function of this screen
            4. Answer to the user's specific question
        """.trimIndent()
    }
    
    private fun buildDocumentPrompt(documentType: DocumentType, userPrompt: String): String {
        val typeSpecific = when (documentType) {
            DocumentType.RECEIPT -> "This is a receipt. Extract items, prices, total amount, merchant name, and date."
            DocumentType.INVOICE -> "This is an invoice. Extract invoice number, date, items, amounts, and vendor information."
            DocumentType.ID_CARD -> "This is an ID card. Extract name, ID number, date of birth, and other visible information."
            DocumentType.FORM -> "This is a form. Extract field names, values, and overall structure."
            else -> "This is a document. Extract all visible text and structure."
        }
        
        return """
            $typeSpecific
            
            User's specific request: $userPrompt
            
            Please provide both the extracted text and structured data where applicable.
        """.trimIndent()
    }
    
    private fun parseScreenshotAnalysis(text: String): Pair<List<String>, List<String>> {
        val uiElements = mutableListOf<String>()
        val textRegions = mutableListOf<String>()
        
        // Simple parsing - in production, this could be more sophisticated
        val lines = text.split('\n')
        var inUiSection = false
        var inTextSection = false
        
        for (line in lines) {
            when {
                line.contains("UI elements", ignoreCase = true) -> inUiSection = true
                line.contains("text", ignoreCase = true) && line.contains("content", ignoreCase = true) -> {
                    inUiSection = false
                    inTextSection = true
                }
                line.startsWith("-") || line.startsWith("‚Ä¢") -> {
                    val element = line.removePrefix("-").removePrefix("‚Ä¢").trim()
                    if (inUiSection) uiElements.add(element)
                    if (inTextSection) textRegions.add(element)
                }
            }
        }
        
        return Pair(uiElements, textRegions)
    }
    
    private fun parseTextRegions(text: String): List<String> {
        // Split text into logical regions/lines
        return text.split('\n').filter { it.isNotBlank() }
    }
    
    private fun parseDocumentData(text: String, documentType: DocumentType): Map<String, Any> {
        val data = mutableMapOf<String, Any>()
        
        // Simple structured data extraction based on document type
        when (documentType) {
            DocumentType.RECEIPT -> {
                // Extract receipt-specific data
                data["raw_text"] = text
                // Add more sophisticated parsing here
            }
            DocumentType.INVOICE -> {
                // Extract invoice-specific data  
                data["raw_text"] = text
                // Add more sophisticated parsing here
            }
            else -> {
                data["raw_text"] = text
            }
        }
        
        return data
    }
    
    private fun getModelPath(baseModel: String): String {
        // In production, this would resolve the actual model file path
        return "/data/data/${context.packageName}/models/$baseModel.gguf"
    }
}

data class LoadedVisionModel(
    val descriptor: MultimodalModelDescriptor,
    val nativeHandle: Long,
    val loadTime: Long
)

data class VisionInferenceResult(
    val text: String,
    val confidence: Float
)

data class VisionConfig(
    val maxImageSize: ImageSize,
    val supportedFormats: List<ImageFormat>
)
```

### 4. Dependency Injection Setup

#### 4.1 Production DI Module
**File**: `core-multimodal/src/main/kotlin/com/nervesparks/iris/core/multimodal/di/MultimodalModule.kt`

```kotlin
@Module
@InstallIn(SingletonComponent::class)
object MultimodalModule {
    
    @Provides
    @Singleton
    fun provideImageProcessor(
        @ApplicationContext context: Context,
        @IoDispatcher ioDispatcher: CoroutineDispatcher
    ): ImageProcessor {
        return ImageProcessorImpl(context, ioDispatcher)
    }
    
    @Provides
    @Singleton
    fun provideMultimodalModelRegistry(
        deviceProfileProvider: DeviceProfileProvider,
        modelCatalogService: ModelCatalogService,
        @ApplicationContext context: Context,
        @IoDispatcher ioDispatcher: CoroutineDispatcher
    ): MultimodalModelRegistry {
        return MultimodalModelRegistryImpl(
            deviceProfileProvider, 
            modelCatalogService, 
            context, 
            ioDispatcher
        )
    }
    
    @Provides
    @Singleton
    fun provideVisionProcessingEngine(
        nativeInferenceEngine: NativeInferenceEngine,
        imageProcessor: ImageProcessor,
        performanceMonitor: PerformanceMonitor,
        @ApplicationContext context: Context,
        @IoDispatcher ioDispatcher: CoroutineDispatcher
    ): VisionProcessingEngine {
        return VisionProcessingEngineImpl(
            nativeInferenceEngine,
            imageProcessor,
            performanceMonitor,
            context,
            ioDispatcher
        )
    }
}
```

## üìã Testing Strategy

### 1. Unit Tests
- **ImageProcessorImpl**: Test image validation, preprocessing, format conversion
- **MultimodalModelRegistryImpl**: Test model loading, compatibility assessment, caching
- **VisionProcessingEngineImpl**: Test inference flow with mock native engine

### 2. Integration Tests  
- **End-to-End Vision Flow**: Real image ‚Üí preprocessing ‚Üí inference ‚Üí results
- **Model Loading**: Verify actual model files can be loaded
- **Performance Testing**: Memory usage, inference speed, cache behavior

### 3. Device Testing
- **Memory Constraints**: Test on low-memory devices
- **Performance Validation**: Benchmark on different device classes
- **Format Support**: Test various image formats and sizes

## ‚úÖ Acceptance Criteria

### 1. Core Functionality
- [ ] Real image preprocessing with Android Bitmap API
- [ ] Actual model loading through native llama.cpp integration
- [ ] Production-quality vision inference with streaming support
- [ ] Comprehensive error handling and recovery

### 2. Performance Requirements
- [ ] Image preprocessing < 200ms for 512x512 images
- [ ] Model loading < 5 seconds on mid-range devices
- [ ] Vision inference < 2 seconds for simple queries
- [ ] Memory usage within device-appropriate bounds

### 3. Quality Standards  
- [ ] 95%+ unit test coverage on new implementations
- [ ] Integration tests covering major user flows
- [ ] Performance benchmarks documented
- [ ] Error scenarios properly handled

### 4. Integration Requirements
- [ ] Seamless replacement of mock implementations
- [ ] No breaking changes to existing interfaces
- [ ] Proper dependency injection configuration
- [ ] Documentation updated with production details

## üîß Implementation Notes

### Critical Dependencies
1. **Native Engine Integration**: Requires functional `NativeInferenceEngine` from core-llm
2. **Model Management**: Needs `DeviceProfileProvider` and `ModelCatalogService` 
3. **Performance Monitoring**: Requires `PerformanceMonitor` for optimization

### Key Considerations
1. **Memory Management**: Vision models are memory-intensive, implement proper caching and cleanup
2. **Error Resilience**: Network failures, model loading errors, out-of-memory scenarios
3. **Performance Optimization**: Device-specific tuning, batch processing, lazy loading
4. **Thread Safety**: Concurrent vision requests, model loading synchronization

### Backward Compatibility
- All existing interfaces remain unchanged
- Mock implementations serve as fallbacks during development
- Graceful degradation when native engine unavailable

---

**Priority**: P1 - Critical for production multimodal capabilities  
**Estimated Effort**: 8-12 days of focused development  
**Risk Level**: Medium - Dependent on native engine stability  
**Success Criteria**: Real vision-language inference working end-to-end with performance targets met